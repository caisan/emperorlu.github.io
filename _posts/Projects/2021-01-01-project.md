---
title: Projects and Files
# permalink: /project/
project: true
tags:
  - Projects
---

项目总结和文件下载

## Projects

- [PingCAP-TiKV](https://github.com/sbh123/wnlokv)
  - 简介：构建一个小型的分布式KV服务器学习。KV服务器可以部署在单个mathine上。它有两个主要部分，服务器端和客户端。服务器是基于grpc-rs、raft-rs和rocksdb-rs的KV服务器；客户端是对服务器的测试。
- [AEP-新介质](https://github.com/emperorlu/Sprint-AEP)
  - 简介：Intel Optane DC Persistent Memory Module (AEP) 是首款商用非易失性内存 (NVM) 产品，可提供与 DRAM 相当的性能，同时提供更大的容量和数据持久性。现有的用 DRAM 替代 NVM 或混合它们的研究要么基于仿真器，要么专注于如何提高写入的能效。不幸的是，真正的 AEP 系统的能效研究较少。基于真实的 AEP，我们观察到，即使消除了类似 DRAM 的刷新能耗，AEP 在不同的性能水平上消耗的能量也有显着差异。具体来说，与没有时间间隔的请求（紧凑）相比，具有时间间隔（分散）的请求在性能和能源效率方面都表现不佳。这种差异性和并行性利用潜力促使我们提出 Sprint-AEP，这是一种面向配备 AEP 的服务器的能源效率调度方法。Sprint-AEP 通过延迟写入请求和预取最热门的数据来完全激活足够的 AEP 来服务大多数请求。剩余的 AEP 将保持在空闲模式，并以低空闲功率来节省能源。此外，我们还利用读取并行性来加速同步和预取过程。与不知道能量的 AEP 使用相比，我们的实验结果表明，Sprint-AEP 可以节省高达 26% 的能量，而性能几乎没有下降。Sprint-AEP 通过延迟写入请求和预取最热门的数据来完全激活足够的 AEP 来服务大多数请求。剩余的 AEP 将保持在空闲模式，并以低空闲功率来节省能源。此外，我们还利用读取并行性来加速同步和预取过程。与不知道能量的 AEP 使用相比，我们的实验结果表明，Sprint-AEP 可以节省高达 26% 的能量，而性能几乎没有下降。Sprint-AEP 通过延迟写入请求和预取最热门的数据来完全激活足够的 AEP 来服务大多数请求。剩余的 AEP 将保持在空闲模式，并以低空闲功率来节省能源。此外，我们还利用读取并行性来加速同步和预取过程。与不知道能量的 AEP 使用相比，我们的实验结果表明，Sprint-AEP 可以节省高达 26% 的能量，而性能几乎没有下降。
- [Sensetime-大集群](../files/商汤科技高校合作年度工作交流会_万继光5.pptx)
  - 简介：面向AI训练平台的存储优化，存储平台基于Ceph构建，主要研究方向：
    - **调研对象存储系统优化方向，明确对象存储性能优化点。**存储部门采用大规模分布式对象存储Ceph系统存储模型和数据，随着存储规模不断扩大和高性能存储设备不断发展，对Ceph系统性能提出更多的挑战，包括高带宽/高IOPS性能要求，数据量非常大且以小文件居多，随机读负载占大多数，集群迁移和扩容更加频繁等等。针对这些挑战，我们调研了相关研究工作，明确对象存储系统优化方向和可行的优化点。
    - **测试和分析对象存储系统Ceph的读性能瓶颈。**现有的Ceph系统并不能充分发挥高性能NVMe SSD的性能，我们进行了大量性能测试和分析，梳理了Ceph系统读写IO路径，定位了读性能瓶颈点。
    - **提升Ceph在NVMe SSD介质上至少30% IOPS性能**。针对读性能瓶颈，我们采取相应的优化措施提升读性能，降低读延迟：包括开发了读优化的元数据KV引擎TridentKV，设计OSD异步读流程，采用高性能SPDK接口访问NVMe SSD。并且TridentKV采用分区存储架构优化了数据迁移导致读性能下降问题。
    - **实现大集群Ceph系统数据分布可控。**Ceph系统采用伪随机Crush数据分布算法，在实际集群中，常常发生数据分布不均匀，扩容引发不可控数据迁移等问题。我们设计了高效数据分布算法，当大集群发生故障和扩容时，实现数据分布过程中业务IO可控，保证集群高可用性。
    - **开发Ceph系统自动参数调优工具。**在大规模分布式系统中，可以配置的参数空间非常庞大，针对不同场景不同业务，人工很难设置最佳参数，使用默认参数往往导致性能欠佳。我们开发了一套Ceph系统自动参数调优工具，针对不同场景不同业务，推荐最优参数，提高集群性能。
- [Sensetime-AI优化](../For AI/2021-12-14-IO Optimization for RL framework.md)
  - NLP大模型存储需求
    - 性能瓶颈分析器
    - Autotuning，根据环境资源情况（包含IB、显存、显卡功率、内存、硬盘io等使用情况）自动并行挑选最优训练策略
    - 超大模型读写，硬件资源选择（Nvme等等）
    - 检索数据库的搭建
  - 强化学习存储需求
    - 实现支持大容量的replay buffer,采用交换到本地ssd的方式，提供一个kv接口的库，在di_store里面
    - 思考如何统一显存，内存，ssd，单机，多机的存储使用方式，提供一个统一的kv库，统一解决问题


## About Papers

- [CCF推荐国际学术会议和期刊目录](https://ccf.atom.im/)

- [国光论文列表](../files/TOP 80列表最新.docx)

- paper search

  - [dblp](https://dblp.org/)

  - [semanticscholar](https://www.semanticscholar.org/)

- [draw.io](https://app.diagrams.net/)

- [overleaf](https://www.overleaf.com/)

- [matplotlib](https://github.com/matplotlib/matplotlib)

## Files

- ppts

  - [MapX](../files/MapX.pdf)

  - [OSDI\'20](../files/osdi20论文分享.pptx)

  - [FAST\'21 && SplitDB](../files/SplitDB和fast21.pptx)

  - FAST'22

