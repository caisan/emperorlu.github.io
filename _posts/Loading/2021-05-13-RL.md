---
title: 'Replica Placement with RL'
tags:
  - Load_Balancing
---

cache



- 缓存算法在某些工作负载下表现良好，但在其他工作负载下却表现不佳
- 缓存算法在某些缓存大小下表现良好，但在其他缓存大小下不一定表现良好



LeCaR使用了遗憾最小化[22,21]，这是一种机器学习技术，允许在缓存失败时动态选择这些策略之一。



确定了四种工作负载原语类型: LRU-friendly, LFU-friendly, scan, and churn.   





分布算法设计

1. 均匀性：分布均匀
2. 一致性：相同输入，相同输出
3. 可靠性/可用性：副本放置，故障域隔离
4. 可扩展性：增/删节点处理，增量迁移
5. 其他：性能指标、分布式CAP等等

With RL

<img src="..\..\photos\RL.png" alt="RL" style="zoom: 33%;" />

- 实现1：RL训练出放置策略，action space即为mapping策略

  - 简单实现，目前支持映射均匀性、一致性和副本机制、增删pg节点（n->m）

    - spcae：各osd的容量状态，{weight0，weight1，...，weightm}
    - action：{osd0，osd1，...，osdm}
    - reward：各osd容量标准 / 前后标准差变化 / 极差
    - 模型：当前实现Q-learning和DQN
    - 实现和效果
      - 多次迭代训练，选择效果最佳的结果
      - 1000pg，10osd，3副本，mapping选择另存：

    <img src="..\..\photos\\image-20210513103328853.png" alt="image-20210513103328853" style="zoom:50%;" />

  - 问题1：如何映射？

    - 方式1：将映射关系建表
    - 方式2：分类问题？区分pg id

  - 问题2：状态过多，空间爆炸；训练精度不足，需要训练很久？

    - 改善模型、改善状态定义...

  - 问题3：如何支持增删osd

    - How？

- 实现2：采用原有的放置策略（hash、crush），RL作为辅助调整，即action调整mapping

  - 简单实现，mapping=crush，支持所有
    - space：各osd的容量状态，{weight0，weight1，...，weightm}
    - action：crush选出来后左右移动？
    - reward：经过调整后，标准差变化

