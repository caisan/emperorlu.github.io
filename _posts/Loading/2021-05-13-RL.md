---
title: 'Learning Replica Placement with RLRP'
tags:
  - Load_Balancing
---

# Learning Replica Placement with RLRP

> **来由同一梦，休笑世人痴。 — 小雪**

[TOC]

## Abstract

## Introduce

1. 分布式/对象存储等等
2. 分布问题：负载均衡、可靠性问题（副本和故障域）、迁移扩容等等
3. 强化学习为解决分布问题带来曙光
4. 我们的方案

## Motivation

### 强化学习

### 问题建模

- 副本放置问题: M个数据存到到N个机器上, 每个数据R个副本在不同机器上, 下图中R=2

  ​	<img src="..\..\photos\load.png" alt="load" style="zoom: 25%;" />
  
- 目标1. 均匀性，每个机器上的数据尽量均匀；

- 目标2. 增量迁移性，增加机器，之前机器将数据迁移到新机器上，保证迁移最小且数据均匀；

- 后续保证: 1. 每个机器上的主副本尽量均匀；3. 机器异构环境

- 具体到Ceph

<img src="..\..\photos\背景ceph.png" alt="load" style="zoom: 25%;" />

### 算法目标

1. 均匀性：分布均匀
2. 一致性：相同输入，相同输出
3. 可靠性/可用性：副本放置，故障域隔离
4. 可扩展性：增/删节点处理，增量迁移
5. 其他：性能指标（负载均衡）、分布式CAP等等

## Design

### With RL 设计

- 环境定义（park）+ 模型实现（tensorflow），基本框架：

<img src="..\..\photos\RL.png" alt="RL" style="zoom: 33%;" />

- 两种思路

  - RL作为映射算法
  - RL作为辅助，调整原有映射算法

#### 实现1：RL训练出放置策略，action space即为mapping策略

- 简单实现，目前支持映射均匀性、副本机制、增删pg节点（n->m）

  - **一致性 、扩容暂不支持**
  - spcae：各osd的容量状态，{weight0，weight1，...，weightm，}
  - action：{osd0，osd1，...，osdm}
  - reward：各osd容量标准 / 前后标准差变化 / 极差
  - policy : 当前状态下选择的action，是训练结果
  - 模型：当前实现Q-learning和DQN
  - 实现和效果
    - 多次迭代训练，选择效果最佳的结果
    - 1000pg，10osd，3副本，mapping选择另存：

  <img src="..\..\photos\image-20210513103328853.png" alt="image-20210513103328853" style="zoom:50%;" />

- 问题1：如何映射？

  - 巨大问题：每次得到状态不一致
  - 方式1：将映射关系建表
  - 方式2：分类问题？区分pg id

- 问题2：状态过多，空间爆炸；训练精度不足，需要训练很久？

  - n ^ m (1000 ^ 10 = 10 ^ 30)
  - 改善模型、改善状态定义...
  - DQN中模型选择：目前使用的MLP，结合RNN、CNN？
  - 训练参数调整，Oﬀline Training

- 问题3：如何支持增删osd

  - 减少节点m：控制action不能等于m，就可以均匀分布到其他osd；但是两次映射差距甚大...

  - 增加节点：space和action都要变，模型需要重新训练？另外重新训练和原映射差距甚大...

  - 难点：**增量迁移怎么定义模型？**

    - 想法1：将两次变化量作为reward

      - 训练难收敛，训练效果差

        <img src="..\..\photos\Load\image-20210514154000569.png" alt="image-20210514154000569" style="zoom: 50%;" />

      - 表格 + RL 再训练 

    - 想法2：多agent，每个osd相当于agent，Multi-agent Data Distribution

      <img src="../../photos/v2-6bf1f5737f9f654929fab84f73cba012_720w.jpg" alt="img" style="zoom: 50%;" /><img src="..\..\photos\image-20210513181111622.png" alt="image-20210513181111622" style="zoom: 80%;" />

      - space：weight
      - action：要 / 不要 / 滚；+ 1 / 0 / -1
      - reward：对每个agent 尽量少加 / 尽量少减；所有，标准差
      - 模型学习困难：Actor-Attention-Critic for Multi-Agent Reinforcement Learning，ICML’19
      - 数据集难以构造，有限样本

    - 想法3：动态环境下的强化学习

      - A Survey of Reinforcement Learning Algorithms for Dynamically Varying Environments，2005
      - 多模态强化学习

#### 实现2：采用原有的放置策略（hash、crush），RL作为辅助调整，即action调整mapping

- 简单实现，mapping=crush，支持所有
  - space：各osd的容量状态，{weight0，weight1，...，weightm}
  - action：crush选出来后左右移动？
  - reward：经过调整后，标准差变化
- 问题 
  - 只能对均匀进行优化，是否影响性能？
  - 是否可以抽象成ML分类问题，RL作为调整辅助？
    - ML分类器 + RL
  - 是否可以做成一种通用框架，可以适用于各种分布算法？



#### 增量迁移设计

- 基本目标：分布均匀，增量迁移

​                  <img src="..\..\photos\数据分布RL1.png" alt="数据分布RL1" style="zoom: 25%;" />                         <img src="..\..\photos\数据分布RL设计.png" alt="数据分布RL设计" style="zoom: 25%;" />

- 实现1：增/减节点重新训练
  - state：包括Map
  - Reward: 标准差 && Map变化最少 
  - 训练时间长、训练效果差
- 实现2：增/减节点单独训练模型
  - 模型定义
    - state：每个节点容量
    - action：迁或者不迁
    - reward：最少迁移量 && 标准差
  - 模型得到每个节点应该迁移量，在Map没冲突的情况下，随机选择
  - 两种方式都是扩容一次，训练一次，性能损失严重
    - Attention，记忆RNN？
- 完全RL
  - **可变状态空间或者动作空间**
    - 对深度强化学习算法来说是一个比较棘手的问题。一旦输入空间确定，就意味着神经网络的输入节点个数确定了下来。如果在学习的过程中，输入节点增多或者减少，这对隐藏层乃至输出层的数据都会产生影响
    - RNN + nas
  - **可变数量的多智能体RL**
    - 结合Atention模型，Evolutionary Population Curriculum for Scaling Multi-Agent Reinforcement Learning，2020

### RLRP 实现

#### RLRP（第一阶段，简单Demo）

完成简单Demo，副本放置+强化学习框架，实现同构环境分布均匀和迁移最小化

问题1：可变环境及模型优化

- 每次增加节点都要重新训练，费时费力
- 随着PG和OSD数量增加，训练越来越困难，训练时间越来越长

问题2：异构环境及负载均衡

- OSD异构环境，OSD权重、硬件性能、负载状况（CPU、IO、网络）等等差异
- 主PG的分布影响读性能，迁移之后主PG分布不是很理想



- 副本放置+强化学习框架，实现**同构环境分布均匀和迁移最小化**

<img src="..\..\photos\RL.png" alt="RL" style="zoom: 33%;" />                      <img src="..\..\photos\load\数据分布RL迁移设计.png" alt="数据分布RL设计" style="zoom: 25%;" />

- 如图2，将均匀分布和迁移分成两个模型进行训练，分别是Agent1和Agent2，两则定义如下

- 均匀分布（Agent1）

  - spcae：各osd的容量状态，{weight0，weight1，...，weightm，}

  - action：{osd0，osd1，...，osdm}

    - 限制：每个PG选K次，每次不一样，K为副本数

  - reward：各osd容量标准 / 前后标准差变化 / 极差

  - policy :  当前状态下选择的action，是训练结果

  - 模型：当前实现Q-learning和DQN

  - 输出：映射表（n->m）

    - 下面是三副本的例子
    - 横向上看：可以得到 PGi —> (OSDk, OSDj, OSDl) 的映射，i = 0 to n；0 <= k, j, l <= m
    - 纵向上看：可以得到每个OSD中PG个数

    |      | OSD0 | OSD1 | OSD2 | OSD3 | ...  | OSDm |
    | ---- | ---- | ---- | ---- | ---- | ---- | ---- |
    | PG0  | 0    | 1    | 0    | 1    | ...  | 1    |
    | PG1  | 1    | 1    | 1    | 0    | ...  | 0    |
    | ...  | 0    | 0    | 0    | 0    | ...  | 0    |
    | PGn  | 0    | 0    | 0    | 1    | ...  | 0    |

- 增量迁移

  - 减少OSD节点（Agent1）

    - 减少OSDm，将OSDm中的节点通过Agent1重新映射
    - 限制1：不能选择action OSDm；限制2：不能选择有副本冲突的
    - 输出：更新映射表

  - 增加OSD节点（Agent2）

    - **action：{0，1，2，3}**

      - 对于每个PG，0、1、2分别表示第1、2、3个副本
      - PGi —> (OSDk, OSDj, OSDl)，0表示从OSDk迁移到新节点，1、2表示迁OSDj、OSDl
      - 3表示不迁

    - 其他都和Agent1一样，效果不错

      <img src="..\..\photos\RL\image-20210527140123158.png" alt="image-20210527140123158" style="zoom:50%;" />

    - 加速收敛：调整学习率、调整网络结构；如果达到某个阈值（比如标准差<=3）,可提前停止训练

- 整体演示：200个PGs，3副本映射到10个OSDs上，再添加一个OSD

  - **Agent1**
    - 输入：PG、OSD个数，副本数；输出：映射表
    - 加速：训练某段时间内std小于阈值，停止训练
    - 主PG：先映射一遍得到主PG，再映射两遍得到两副本

  <img src="..\..\photos\RL\test1_osd.png" alt="test1_osd" style="zoom: 67%;" /><img src="..\..\photos\RL\数据分布.png" alt="test1_osdzhu" style="zoom: 67%;" />

  <img src="..\..\photos\RL\test1_st.png" alt="test1_st" style="zoom: 67%;" /><img src="..\..\photos\RL\test2_st.png" alt="test2_st" style="zoom:67%;" />

  - **Agent2**
    - 输入：映射表，新增OSD个数，副本数；输出：映射表
    - 加速：训练某段时间内std小于阈值 / 新增OSD数量最多，停止训练
    - 主PG：设置action 0是迁移主PG，在Reward中添加主PG均衡奖励，暂时**训练不收敛**
      - 先迁移主PG，也就是action只能选择0或者3；再迁移其他，action只能选择1，2，3:**效果没那么好**

  <img src="..\..\photos\RL\数据迁移.png" alt="test2_osd" style="zoom:67%;" /><img src="..\..\photos\RL\test1_osdzhu.png" alt="test_osdzhu" style="zoom:67%;" />

- 模型的比较

  - ing

- 后续

  - 问题
    - 迁出不可能迁回，考虑保留一段时间前映射表
    - **每次增加节点都要重新训练**Agent1和Agent2，费时费力
      - **还是要解决RL环境变化的问题：可变状态和动作空间**
      - Atention模型，结合上一次训练结果
    - 随着PG和OSD数量增加，训练越来越困难，训练时间越来越长
    - 迁移主PG需要重新考虑一下
    - 要考虑其他负载均衡、异构因素

  - 模型优化
    - **Attention Mechanism**
    - RL: Value: Sarsa\Q-learning\DQN... vs. Policy: Policy Gradients\Actor-Critic\A3C\DDPG\PPO...
    - Multi-Agent\Signle-Agent Dynamically Varying Environments
    - NN: MLP\CNN\DNN\RNN\LSTM...

  - 负载均衡
    - OSD之间不同权重，PG大小不均设计
      - 状态中添加PG大小
      - 改Reward计算方法：单位权重（PG数/权重）方差
    - 异构环境设计（Agent3）
      - OSD权重、硬件性能、负载状况（CPU、IO、网络）等等差异
      - Reward目标可能要改成性能
    - 主PG均衡设计
      - Agent1：每次映射是均匀的，先映射一遍得到主PG，再映射一遍得到两副本
      - Agent2：设置action 0是迁移主PG，在Reward中添加主PG均衡奖励

Attention Mechanism

- **从关注全部到关注重点** 
- ing 
- Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation

<img src="..\..\photos\RL\image-20210528145734284.png" alt="image-20210528145734284" style="zoom:50%;" />

#### RLRP（第二阶段，模型优化）

- 模型优化

  - **问题1：模型选择**

  - RL: **Value**: Sarsa\Q-learning\ **DQN**... vs. **Policy**: Policy Gradients\Actor-Critic\A3C\ **DDPG**\PPO...

  - **问题2：OSD、PG增加训练困难**

    - **OSD增加？**
    - 不考虑PG差异情况下
      - 从绝对状态改为相对状态，[100,121,101]  == [0,21,1]，每次将状态组所有元素减去最小的值，得到当前状态
      - 设置一个训练区间，比如1000，每次迭代1000 steps 

  - **问题3：迁出不能迁回，每次映射不一致**

    - 通过保存模型和固定参数，可以保证每次映射结果一致，可以考虑去掉映射表？

  - **问题4：副本选择**

    - 选择action时控制三次选择不能重复，但是在test中模型和参数固定，同样的状态选择是一定的，不能重复的会导致选不出来
    - 计算reward的时候，出现重复给予很大的惩罚，奖惩控制不当出现畸形选择，一直固定选某几个
    - 最终方案：因为模型每次是选择Q值最大的action
      - 选Q值前三的作为三组action 
      - 先选Q值最大的作为action，选其他副本的action和以前某次一样，则选Q值次大的作为替补

  - **问题5：主副本的均衡？**

    - 先跑一遍模型选出主副本
    - 再根据上述副本选择策略跑一遍选出其余副本
  
  - **问题6：扩容之后，新增节点怎么选择？**
  
    - 迁移后的状态作为初始状态，训练新模型
  
  - 演示效果
  
    - 训练的模型：place.ckpt（目前训练出来最完美的模型）
  
      - DQN 模型
      - 以100个pg为训练区间
        - 这个训练效果是非常重要的，设置无限制训练迭代，当连续十次达到满意结果(标准差小于1)，停止训练
        - total episode: 1379 ; cost time:  714.2311611175537 s  （11.9分钟）
        - 三副本 total episode: 631 ; cost time:  985.1475253105164 s （16.4分钟）
      - 模型大小 52K = 8K （ckpt.data ）+  4K （.index） + 40K （.meta）
      - 可以测试随意多少个pg
    
    - 测试：10000 pg，拆分成100个训练区间进行测试
    
      - std：0.7745966692414834； cost time:  4.35152244567871094 s
    
      <img src="..\..\photos\RL\test3_osd.png" alt="test3_osd" style="zoom:67%;" /><img src="..\..\photos\RL\test3_st.png" alt="test3_st" style="zoom: 67%;" />
    
      - 三副本 std：0.9213；cost time:  12.70314908027649 s
    
      <img src="..\..\photos\RL\test4_osdzhu.png" alt="test4_osd" style="zoom:67%;" /><img src="..\..\photos\RL\image-20210602195301847.png" alt="image-20210602195301847" style="zoom: 50%;" />
    
  - 数据迁移：
  
    - 训练 total episode: 992； cost time:  879.645852804184s （14.7分钟）
    - 测试 cost time:  1.1143670082092285；std: 0.44536177141512334
  
    ​		<img src="..\..\photos\RL\test_move2.png" alt="test_move2" style="zoom:67%;" /><img src="..\..\photos\RL\test_move.png" alt="test_move" style="zoom: 67%;" />

##### RLRP（第2.5阶段，OSD增加）

- 问题：
  - OSD增加，每次需要重新训练
    - 10 osd：14.4分钟；11 osd：18.3分钟
  - 迁移需要训练agent2，之后的状态作为初始状态，又要训练新模型
  
- 模型统一
  - 定义一个大的状态和动作范围
  - action：老骥伏枥，志在千里
    - 有老，则是老选择，新节点；没老，则是所有位置的选择
    - 有老，某个副本选了新，后面副本全选老；没老，该怎么选怎么选
  
- 模型调整（参考openai five里面提出的**surgery**，[Dota 2 with Large Scale Deep Reinforcement Learning](https://arxiv.org/pdf/1912.06680.pdf)，66页）

  - 基本思路（大致翻译，具体看原文）

    - 改变神经网络结构的参数量：假设我们有输入变量 ![[公式]](https://www.zhihu.com/equation?tex=x) ，其维度为 ![[公式]](https://www.zhihu.com/equation?tex=d_x) ，经过一个激活函数后得到向量 ![[公式]](https://www.zhihu.com/equation?tex=y%3DW_1x%2BB_1) ，其维度为 ![[公式]](https://www.zhihu.com/equation?tex=d_y) ，然后再经过一层全连接层得到向量 ![[公式]](https://www.zhihu.com/equation?tex=z%3DW_2y%2BB_2) ，其维度为 ![[公式]](https://www.zhihu.com/equation?tex=d_z) 。如果需要增加向量 ![[公式]](https://www.zhihu.com/equation?tex=y) 的参数量，增加其维度从 ![[公式]](https://www.zhihu.com/equation?tex=d_y) 到 ![[公式]](https://www.zhihu.com/equation?tex=%5Chat%7Bd%7D_y) ，那会导致 ![[公式]](https://www.zhihu.com/equation?tex=W_1%2CB_1%2CW_2) 的维度都发生改变： ![[公式]](https://www.zhihu.com/equation?tex=+%5Chat%7BW%7D_1+%3D++++++%5Cbegin%7Bbmatrix%7D++++W_1+%5C%5C++++R%28%29+%5C++++%5Cend%7Bbmatrix%7D++%5Ctext%7B+++++++%7D++++%5Chat%7BB%7D_1+%3D++%5Cbegin%7Bbmatrix%7D++++B_1+%5C%5C++++R%28%29+%5C++++%5Cend%7Bbmatrix%7D++++++%5Ctext%7B+++++++%7D++++%5Chat%7BW%7D_2+%3D+++++%5Cleft%5B+++++%5Cbegin%7Barray%7D++++%26W_2+%26+0+%5C++++%5Cend%7Barray%7D++++%5Cright%5D+++) 这里 ![[公式]](https://www.zhihu.com/equation?tex=R%28%29) 指的是随机初始化，经过这个操作后，可以发现 ![[公式]](https://www.zhihu.com/equation?tex=%5Chat%7BW%7D_1) ， ![[公式]](https://www.zhihu.com/equation?tex=%5Chat%7BB%7D_1) 的前 ![[公式]](https://www.zhihu.com/equation?tex=d_y) 个权重和 ![[公式]](https://www.zhihu.com/equation?tex=W_1) ， ![[公式]](https://www.zhihu.com/equation?tex=B_1) 是完全一致的，随机初始化保证了新加入的权重参数进行优化时可以获得足够的梯度信息，同时， ![[公式]](https://www.zhihu.com/equation?tex=%5Chat%7BW%7D_2) 新增加的权重参数进行置零操作，保证了前面激活部分新增加的变量不影响到全连接层的输出，只将部分的权重置零也不会影响后续的训练。值得注意的是，在LSTM的网络结构中，由于单元之间是循环前后相连的，没有办法如同上面的全连接层一样做到新旧权重参数的完全隔离，如果进行随机初始化，会导致原有模型的能力下降，要是置零，则会导致梯度信息丢失，在实际操作中，采用的方式是两者的综合，即采用一个很小的随机值做初始化，在避免丢失梯度信息的同时最大程度的减少对旧模型能力的影响。
    - 改变观测状态：surgery的一个主要部分是增加了许多新的观测状态，放在模型训练上，可以认为是模型的输入发生了变化，游戏的原始状态是经过一系列的转化和编码操作之后才输入到模型的，因此可以理解为原始的输入并没有发生变化，发生变化的只是对原始输入进行编码的部分。因此只要可以保证 ![[公式]](https://www.zhihu.com/equation?tex=%5Cforall+s+%5Ctext%7B++%7D+%5Chat%7B%5Cpi%7D_%7B%5Chat%7B%5Ctheta%7D%7D%28%5Chat%7BE%7D%28S%29%29%3D%5Cpi_%7B%5Ctheta%7D%28E%28S%29%29) ，这里 ![[公式]](https://www.zhihu.com/equation?tex=S) 是原始游戏状态，而 ![[公式]](https://www.zhihu.com/equation?tex=E%28S%29) 则是编码之后的输入，**即新模型对扩展后的编码状态的能力水平相比旧模型并没有下降，然后套用上述1的方法，将新增部分的编码对应权重做置零操作后进行训练即可**。
    - 改变环境或动作空间：上述surgery的改动部分还涉及游戏版本的更新，也包括了新增动作，将原来通过脚本完成的操作改为由模型控制。明显的与前两种类型不同，这里涉及到学习到的策略函数 ![[公式]](https://www.zhihu.com/equation?tex=%5Cpi) 的改变，因此在训练过程中采用类似“anneal”的策略。**在环境改变后，将训练对局中新环境的比例从0%（即完全旧环境）慢慢上升到100%（过渡到新环境)，来避免环境突变时导致模型能力出现陡变**，同时如果发现模型的Trueskill分在“anneal”过程中出现下降，则进行回退并减慢“anneal”的速度。
    - 模型删减：处于保证模型在surgery后能力不变的考虑，很多删减变量或模型参数的需求没有被考虑在内，这些被废除的变量暂时还是被保存在模型内。
    - 模型训练重启：在模型经过修改并重启训练之后，优化器中的梯度动量信息可能会带来问题，由于参数空间发生了变化，可能会导致梯度优化方向发生改变而进一步影响训练。因此在重启之后的一小段时间内，优化器的学习率被置零，几小时后等所有的rollout对局稳定后才开始正常的训练。另外，surgery不仅需要保证对当前训练的最新版本模型有效，还需要保证对所有的历史模型而言都有效，否则会导致对战时历史模型能力在surgery后出现下降，影响训练效率。

  - 尝试一下，10个OSD增加一个，此时state从10维变成11维，action从10变成11

    - DQN的NN，MLP

    <img src="..\..\photos\RL\MLP.png" alt="test_move2" style="zoom: 25%;" />

    - 网络三层，输入，隐藏层（20节点） ，输出层

       W1 = [self.state_dim,20]， b1 = [20]

       W2 = [20,self.action_dim])，b2 = [self.action_dim])

    - 读取模型，在原模型参数上增加维度 ：原来的值加上新增维度0值

      ```python
      if add:
          self.saver.restore(self.session, path)
          variable_names = [v.name for v in tf.compat.v1.trainable_variables()]
          [W1_old, b1_old, W2_old, b2_old] = self.session.run(variable_names)
      
          W1_add_zero = tf.zeros((add,20))
          W2_add_zero = tf.zeros((20,add))
          b2_add_zero = tf.zeros(add)
      
       	W1 = tf.Variable(tf.concat([W1_old, W1_add_zero], 0))
          b1 = tf.Variable(b1_old)
          W2 = tf.Variable(tf.concat([W2_old,W2_add_zero], 1))
          b2 = tf.Variable(tf.concat([b2_old,b2_add_zero], 0))
      ```

    -  测试对比，达到相同效果（连续10次迭代标准差<1）

      - cost time: **1096.7204835414886** s (原来) 
      - cost time:  **83.1434931755065** s (优化后) cost time:  **74.44707727432251** s ; cost time:  **81.61742353439331**s
      - 快了 92%

    - 补0改成随机

      ```python
      W1_add_random = tf.random.truncated_normal(W1_add_zero.shape)
      W2_add_random = tf.random.truncated_normal(W2_add_zero.shape)
      b2_add_random = tf.random.truncated_normal(b2_add_zero.shape)
      
      W1 = tf.Variable(tf.concat([W1_old, W1_add_random], 0))
      b1 = tf.Variable(b1_old)
      W2 = tf.Variable(tf.concat([W2_old,W2_add_random], 1))
      b2 = tf.Variable(tf.concat([b2_old,b2_add_random], 0))
      ```

      - cost time:  **264.1176526546478** s; cost time:  **399.5544981956482** s; cost time:  **237.48365426063538** s

    - 增加10个OSD，总过20个OSD，训练区间增加为200PGs

      - 原训练时间：cost time:  （DQN未知，要求太严苛下，跑了一晚上没训练出来）DDPG+5MLP **3小时24分钟**

      - 优化后（10->11->..->20）训练时间：**2829.1229819754** s (约47分钟)，快了**78%**

        - 类似于**Curriculum learning**，Self-Paced Learning for Latent Variable Models，NeurIPS’10

      - 高维度状态可以采取此策略

        | 步数                       | 1        | 2    | 3    | ...  |
        | -------------------------- | -------- | ---- | ---- | ---- |
        | 10-11, 10-12,10-13         | 81s      | 198s | 669s |      |
        | 11-12                      | 84s      |      |      |      |
        | 12-13-14-15-16-17-18-19-20 | 100~500s |      |      |      |

- Attention

  - NLP的seq2seq问题，encoder+decoder，attention模型用于decoder过程中，它改变了传统decoder对每一个输入都赋予相同向量的缺点，而是根据单词的不同赋予不同的权重

    <img src="..\..\photos\RL\image-20210611144652326.png" alt="image-20210611144652326" style="zoom:33%;" />

  - 核心观点：不同节点作用/影响不同，给每个节点加一个权重

    ​																				<img src="..\..\photos\RL\image-20210611144619383.png" alt="image-20210611144619383" style="zoom: 50%;" /> 

- 尝试一下，为隐藏层输出加一个权重Ai，在训练过程中更新

```python
def build(self,inputshape):
   	assert len(inputshape) == 3
    self.W = self.add_weight(name='attr_weight',
        shape=(inputshape[1], inputshape[2]),
        initializer='uniform',
        trainable=True)
    self.b = self.add_weight(name='attr_bias',
        shape=(inputshape[1],),
        initializer='uniform',
        trainable=True)
    super(attention_layers, self).bulid(inputshape)
```

- 还存在bug

  

#### RLRP（第三阶段，异构环境）

- OSD权重的差异

  - 不同weight：最简单的做法，state_i={pg数/weight_i}

    - 【例子1】 weight: [2, 3, 5, 4, 7, 2, 2, 2, 2, 2] ，训练区间是100

    - 训练 total episode: 548 ; cost time:  626.9669451713562 s

    - 测试  std: 0.4696938456699067; cost time:  4.37903094291687

    - 【例子2】weight: [21, 35, 5, 44, 7, 54, 25, 12, 2, 22]，训练区间是1000

    - 训练  total episode: 880 ; cost time:  5182.777556657791 s

    - 测试 servers: std: 0.48989794855663565；cost time:  4.458923101425171

      <img src="..\..\photos\RL\test5_weight1.png" alt="test5_osd" style="zoom:67%;" /><img src="..\..\photos\RL\test5_weight2.png" alt="test5_weight" style="zoom:67%;" />

  - soga

    - 不小心当测试不同权重的时候，使用了原模型 place.ckpt

    - 【例子1】std: 0.8; cost time:  4.831289529800415

    - 【例子2】std: 1.077032961426901； cost time:  4.3217644691467285

      <img src="..\..\photos\RL\test6_weight.png" alt="test6_weight" style="zoom:67%;" /><img src="..\..\photos\RL\test6_weight2.png" alt="test5_weight2" style="zoom:67%;" />

    - 仔细思考一下，也合理，跟状态转换有关

      - （pg0, pg1, pg2, ,,, ,pgn） ->  (pg0/weight0, pg1/weight1, ,,, ,pgn/weightn)
      - place.ckpt 模型可以保证 (pg0/weight0, pg1/weight1, ,,, ,pgn/weightn) 的均衡，于是就有了这样的分布
      - 但是比单独训出来的效果差一点点

  - 这样的话：现在得到的 place.ckpt 模型可以解决不同weight的情况 

- OSD异构环境

  - 存储设备差异：NVM、SSD、HDD
  - 网络、IO、后台等等资源差异

- 对象及PG的差异

  - 访问频率和访问模式：主要考虑的是obj -> pg的映射了
  - 大小，id

#### RLRP（第四阶段，系统构建）

- In Ceph or others



### 总结

<table>
    <tr>
        <td><b>阶段</b></td> 
        <td colspan="5"><b>目标</b></td>   
   </tr>
    <tr>
        <td width="22.5%" rowspan="2">一、基本功能，简单Demo</td>    
        <td width="15%" >分布均匀</td>  
        <td width="16%" >最小化迁移</td> 
        <td width="13%" >高效查询</td> 
        <td width="13%" >主副本</td> 
        <td >模型简单训练快速</td> 
    </tr>
    <tr>
        <td >优</td>  
        <td >优</td> 
        <td ><b>中</b>，需要查表</td> 
        <td >优</td> 
        <td ><b>差</b>，随着OSD和PG增加，训练时间激增</td>  
    </tr>
    <tr>
        <td rowspan="2">二、训练加速，模型调优</td>    
        <td >模型选择与优化</td>  
        <td >OSD规模增大训练困难</td> 
        <td >PG数量增加训练困难</td> 
        <td >映射不一致</td> 
        <td >扩容之后副本选择</td> 
    </tr>
    <tr>
        <td >优，DDPG</td>  
        <td ><b>差</b>，在考虑加入Attention机制</td> 
        <td >优，训练区间</td> 
        <td >优，模型保存与参数固定</td> 
        <td ><b>中</b>，原状态下重新训练</td>  
    </tr>
    <tr>
        <td rowspan="2">三、多态处理，适应异构</td>    
        <td >不同权重</td>  
        <td >不同介质</td> 
        <td >不同负载环境</td> 
        <td >对象大小、ID等特征</td> 
        <td >对象访问频率和访问模式</td> 
    </tr>
    <tr>
        <td >优</td>  
        <td ><b>无</b></td> 
        <td ><b>无</b></td> 
        <td ><b>无</b></td> 
        <td ><b>无</b></td>  
    </tr>
    <tr>
        <td rowspan="2">四、系统构建，完善接口</td>    
        <td >封装到Ceph</td>  
        <td >系统搭建</td> 
        <td >提供API</td> 
        <td >系统功能性测试</td> 
        <td >性能测试</td> 
    </tr>
    <tr>
        <td ><b>无</b></td>  
        <td ><b>无</b></td> 
        <td ><b>无</b></td> 
        <td ><b>无</b></td> 
        <td ><b>无</b></td>  
    </tr>
</table>
- 两种方向

  - 提出一种分布算法，（MIT，André Brinkmann，基于balls-in-bins模型对分布算法提出四条准则，2002年）

    - Capacity Efficiency and Fairness. A scheme is called capacity efficient if it allows us to store a near-maximum number of data blocks. Fairness describes the property that the number of balls and requests received by a bin are proportional to its capacity. （容量效率和公平性）

    - Time Efficiency. Computing the position of an object with a low time and space complexity A scheme is called time efficient if it allows a fast computation of the position of any copy of a data block without the need to refer to centralized tables. Schemes often use smaller tables that are distributed to each node that must locate blocks. 

      （时间效率）

    - Compactness. We call a scheme compact if the amount of information the scheme requires to compute the position of any copy of a data block is small (in particular, it should only depend on n—the number of bins). （紧凑性）

    - Adaptivity. We call a scheme adaptive if it only redistributes a near-minimum amount of copies when new storage is added in order to get back into a state of fairness.  （自适应）

  - 系统

- 效果

  - 算法
    - 从模型上优化，解决OSD增加的问题
    - 高效性：训练需要耗时、需要查表
  - 系统
    - 性能和均匀的权衡，考虑异构环境
    - 放置策略：根据环境最优放置
    - 动态策略：在初始放置的条件下，根据环境最优迁移调整

- 系统

  - collecter：收集并处理指标数据
  - model：训练模型
  - controller：应用策略

## Evaluation

- 测试平台：Cloudsim、COSBench、fio/rados benchmark

  https://github.com/intel-cloud/cosbench

- Ceph

- 真实数据

## Related Work

- 分布算法（见[load balancing.md](2021-04-12-数据分布和负载均衡.md)）

- 强化学习（见[RL.md](2021-04-23-replica placement with RL.md)）

- 强化学习存储研究（见[RL.md](2021-04-23-replica placement with RL.md)）

- 很好的学习教程： https://www.zhihu.com/column/c_1215667894253830144 和 莫凡 https://mofanpy.com/

### RL

#### Evolutionary Population Curriculum for Scaling Multi-Agent Reinforcement Learning

- 主题多智能体缩放问题，ICLR‘20，OpenAI，Facebook，CMU等等
- 

### Placement with RL 

#### Towards Self-Managing Cloud Storage with Reinforcement Learning

- 主题Ceph-RL，机构University of Texas at San Antonio，https://ieeexplore.ieee.org/abstract/document/8789906

- IEEE International Conference on Cloud Engineering (IC2E) 2019，未列入ccf推荐，算云计算较好会议

- 提出：In Ceph，**Reinforcement Learning Based Adaptive Load Balancing and Data Migration**

  <img src="..\..\photos\Load\021800a034-fig-1-source-large.gif" alt="021800a034-fig-1-source-large" style="zoom: 50%;" />            <img src="..\..\photos\Load\021800a034-fig-5-source-large.gif" alt="021800a034-fig-5-source-large" style="zoom: 50%;" />

- performance hotspots

  - 某些osd比其他性能更差，主要原因是硬件异构性、各种后台干扰
  - Ceph性能下降：由于CPU、磁盘I/O和各种工作负载混合的网络使用不平衡

- 负载分析

  - COSBench的四种不同的工作负载混合

    ![021800a034-table-1-source-small](..\..\photos\Load\021800a034-table-1-source-small.gif)

  - 性能热点测试

    <img src="..\..\photos\Load\021800a034-fig-2-source-small.gif" alt="021800a034-fig-2-source-small"  />

    

    - 8节点OSD，2个OSD有后台干扰
    - 从第4个采样间隔开始，后台作业会导致整个Ceph存储节点中CPU，磁盘I / O和网络资源的利用率出现各种程度的不平衡
      - CPU利用率
      - ％iowait：CPU空闲、并且有仍未完成的I/O请求
      - 网络利用率

    ![021800a034-fig-3-source-small](..\..\photos\Load\021800a034-fig-3-source-small.gif)

    - 在存储节点之间CPU利用率高度不平衡的情况下，COSBench工作负载的性能基本不受影响
    - **在网络利用率不平衡的情况下，读工作量大的工作负载A的性能下降很大**(平均读响应时间增加了6倍)。但是，其余的工作负载没有受到影响。这是由于工作负载B，C和D中存在大量写操作。由于每个写操作都涉及在网络上所有OSD副本上进行写操作，因此，工作负载B，C和D已经引起Ceph OSD之间的高网络利用率，并且即使没有后台作业的网络干扰，也遭受较大的网络延迟。
    - 在后台作业的I / O干扰的情况下，我们发现，就％iowait而言，即使**少量的存储节点之间的不平衡也会导致所有COSBench工作负载的性能显着下降**。

- 调控负载均衡旋钮

  - primary affinity value：决定成为主OSD的概率

    - 默认情况下，所有的osd主关联值为1，降低一个osd主关联值可以减少它所服务的读工作量。因此，可以利用primary affinity来调整Ceph osd的读工作负载服务比例。
    - 但是只能影响读性能，写性能没影响

  - OSD weights：决定了OSD中pg数量

    - 权重对读写性能都有影响，但改变权重会导致数据在OSD间迁移，带来相关的开销

      <img src="..\..\photos\Load\021800a034-fig-4-source-small.gif" alt="021800a034-fig-4-source-small" style="zoom:50%;" />

    - 上图可以看出，第二个采样间隔（时间60秒）开始，降低了运行后台作业的两个存储节点的OSD权重

    - 由于OSD重新加权导致数据移动，因此COSBench工作负载A的平均读取响应时间最初会增加

    - 但是，从采样间隔5开始，性能显着提高。这是因为数据从两个性能不佳的OSD迁移到其余OSD，将大部分后续读取请求转移到性能良好的OSD

- 基于强化学习的负载均衡和数据迁移

  - 检测性能指标，通过调整primary affinity和weight来实现负载均衡，提升性能
  - state：st=(iowait,rtps,wtps, net)t
    - %iowait值、每秒磁盘读请求数、每秒磁盘写请求数、网络利用率
  - action：调控
    -  I/O-based affinity control
    - I/O-based OSD reweight
    -  network-based affinity control
    - network-based OSD reweight 
    - take no action (no-op)
  - reward
    - 读heavy负载：平均读响应时间的倒数
    - 写heavy负载：平均写响应时间的倒数
    - 读-写均衡负载：2/（平均读响应时间 + 平均写响应时间)

- 总结&&问题

  - 就是提供了一种类似upmap的工具，来调整集群使得性能最优
  - 数据迁移频繁（论文认为不要紧），未考虑节点容量
  - 本身的数据监控采集，训练，调控会不会占用资源，影响性能
  - 负载类型如何判定

- 负载均衡 

  - 节点磁盘容量（数据均衡），数据访问（read/write）负载（负载均衡）
  - 设计主pg均衡，其实就是在考虑负载均衡
  - 我们在考虑数据均衡时，是否要考虑负载均衡

    - cpu、io、网络等等：这种负载均衡的文章特别多
    - 数据迁移量
  - 分布式异构环境下的负载均衡

#### Device Placement Optimization with Reinforcement Learning  

- 主题：RL-Tensorflow中训练操作在各个设备（CPU、GPU）上的分配优化

- ICML'17, Azalia Mirhoseini, MIT/Google；Jeff Dean, Google

- 问题抽象，M->D

  <img src="..\..\photos\load\image-20210526163228832.png" alt="image-20210526163228832" style="zoom:50%;" />

  - 寻求最小化执行时间r(P)时，对r(P)的直接优化会导致两个主要问题

    - 在训练过程的开始，由于不好的位置采样，r(P)的测量可能是有噪声的，导致不适当的学习信号
    - 随着RL模型的逐渐收敛，被采样的位置变得更加相似，相应的运行时间之间的差异较小，导致训练信号的可分辨性较差

  - 问题特点：每个操作之间是有联系的，上个操作会影响下个操作，比较适合用RNN

  - 模型

    - **attentional RNN / LSTM + Policy Gradients**  
    - 学习参数使用 Adam optimizer ,基于通过REINFORCE方程计算的policy gradients 
    - 输入
      - type：MatMul or conv2d  
      - output shapes、adj
    - Attentional LSTM
      - Attention Mechanism：解决变长的输入X映射到一个变长输出Y的问题

    <img src="..\..\photos\RL\image-20210526171831851.png" alt="image-20210526171831851" style="zoom: 50%;" />

    - Distributed Training  

      <img src="..\..\photos\RL\image-20210528143954156.png" alt="image-20210528143954156" style="zoom:50%;" />

#### A Hierarchical Model for Device Placement

- ICLR'18,  Hierarchical Attention Mechanism

  <img src="..\..\photos\RL\image-20210528144116290.png" alt="image-20210528144116290" style="zoom:67%;" />

#### Placement Optimization with Deep Reinforcement Learning

- ISPD'20, Azalia Mirhoseini, MIT/Google

<img src="..\..\photos\RL\image-20210528144145208.png" alt="image-20210528144145208" style="zoom: 67%;" />

### Placement Balance

#### Mirador: An Active Control Plane for Datacenter Storage

- Fast'17，Coho Data，The Load Rebalancing Problem

- 企业扩展存储产品中的动态放置服务，考虑存储系统的性能、故障响应和工作负载适应相关的多维布局目标

- Observe

  - 静态特性，如集群拓扑结构(例如，设备和节点的数量、它们的网络链路容量和用户定义的故障域)
  - 动态特性，如当前的空闲空间和设备的IO负载以及网络端口的利用率

  <img src="..\..\photos\RL\image-20210604101044823.png" alt="image-20210604101044823" style="zoom:67%;" />

#### PRS: A Pattern-Directed Replication Scheme for Heterogeneous Object-Based Storage

- TC‘19, IEEE Transactions on Computers，A类期刊，中科院大学，挂了Shuibing He

- 面向对象存储，高效的异构存储系统复制方案

- 通过计算对象距离将具有 **I/O 相关性的对象聚合到对象组**中，并根据识别的应用程序数据访问模式对分组对象进行复制，使用**伪随机算法**通过考虑存储设备性能和容量特性来优化副本放置

  <img src="..\..\photos\RL\zhou1-2954089-small.gif" alt="图 1. - 典型的基于对象的存储系统中带有复制的文件划分。 " style="zoom: 50%;" /><img src="..\..\photos\RL\tc-gagraphic-2954089.jpg" alt="img" style="zoom: 80%;" />
  
- 背景

  - 单个文件被划分为多个 4 MB 对象。对于每个对象，都有一个与之对应的对象ID，即*object0*、*object1*、*object2*等。对象 ID 唯一标识用于定位的对象。所有对象及其副本分布在不同的节点/设备上进行存储。
  - 要考虑异构存储，对象访问模式

- 设计（PRS ）

  - 基于Sheepdog，异构的存储介质

  - 对象第一和第二副本按照Sheepdog的默认数据布局（基于hash）放置

  - 对象的其余副本将通过模式分析使用 PRS 创建

    - tracing collector
      - 1) 对象 ID，2) 对象中的起始偏移量，3) 访问的大小，4) 访问时间，以及 5) 操作（例如，读或写）
      - 在应用程序执行期间在线收集对象 I/O 跟踪，在分布式存储系统中，每个存储节点都会生成一个跟踪文件，其中包含该节点接收或转发的所有 I/O 请求。跟踪文件用于数据访问模式分析和对象重组以在节点中进行复制。
    - tracing analyzer
    - object reorganization
    - distribution algorithm
    - data replicator

  - 伪随机数算法来优化异构环境中的副本布局

    - SPOCA、ASURA；SUORA（同作者）
    - consistent hash、crush、random slicing（TOS14）

    <img src="..\..\photos\RL\image-20210608102233310.png" alt="image-20210608102233310" style="zoom: 40%;" /><img src="..\..\photos\RL\image-20210608102248070.png" alt="image-20210608102248070" style="zoom: 50%;" /><img src="..\..\photos\RL\7549423-fig-4-source-small.gif" alt="7549423-fig-4-source-small" style="zoom: 50%;" />

#### Random Slicing: Efficient and Scalable Data Placement for Large-scale Storage Systems、

- TOC’14，Barcelona Supercomputing Center  

- 基于动态区间的对象分布算法：该算法将对象映射到(0,1)区间，并将区间划分为多个子区间，每个节点拥有和其权值成正比的多个子区间，以此为依据进行数据映射

  <img src="../../photos/image-20200911104127196.png" alt="image-20200911104127196" style="zoom: 33%;" />

#### Semantics of caching with SPOCA: a stateless, proportional, optimally-consistent addressing algorithm  

- ATC‘2011，Yahoo! Inc  

  <img src="..\..\photos\RL\image-20210608104615955.png" alt="image-20210608104615955" style="zoom: 67%;" />

#### ASURA: Scalable and Uniform Data Distribution Algorithm for Storage Clusters  

- 2013年，未发表，引用100多

  

> **人类总会选择最安全、最中庸的道路前进，群星就会变成遥不可及的幻梦。— 小夫**