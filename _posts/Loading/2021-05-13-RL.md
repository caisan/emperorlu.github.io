---
title: 'Replica Placement with RL'
tags:
  - Load_Balancing
---

# Learning Replica Placement with RLRP

> **人类总会选择最安全、最中庸的道路前进，群星就会变成遥不可及的幻梦。— 小夫**

[TOC]

## Abstract

## Introduce

1. 分布式/对象存储等等
2. 分布问题：负载均衡、可靠性问题（副本和故障域）、迁移扩容等等
3. 强化学习为解决分布问题带来曙光
4. 我们的方案

## Motivation

- 强化学习

- 问题建模

  - 副本放置问题: M个数据存到到N个机器上, 每个数据R个副本在不同机器上, 下图中R=2

    ​	<img src="..\..\photos\load.png" alt="load" style="zoom: 25%;" />

- 目标: 1. 每个机器上的数据尽量均匀

- 后续保证: 2. 每个机器上的主副本尽量均匀；3. 机器异构环境

- 具体到Ceph

<img src="..\..\photos\背景ceph.png" alt="load" style="zoom: 25%;" />

- 算法目标
  1. 均匀性：分布均匀
  2. 一致性：相同输入，相同输出
  3. 可靠性/可用性：副本放置，故障域隔离
  4. 可扩展性：增/删节点处理，增量迁移
  5. 其他：性能指标、分布式CAP等等

## Design

With RL

- 环境定义（park）+ 模型实现（tensorflow），基本框架：

<img src="..\..\photos\RL.png" alt="RL" style="zoom: 33%;" />

- 两种思路

  - RL作为映射算法
  - RL作为辅助，调整原有映射算法

#### **实现1：RL训练出放置策略，action space即为mapping策略**

- 简单实现，目前支持映射均匀性、副本机制、增删pg节点（n->m）

  - **一致性 、扩容暂不支持**
  - spcae：各osd的容量状态，{weight0，weight1，...，weightm，}
  - action：{osd0，osd1，...，osdm}
  - reward：各osd容量标准 / 前后标准差变化 / 极差
  - policy : 当前状态下选择的action，是训练结果
  - 模型：当前实现Q-learning和DQN
  - 实现和效果
    - 多次迭代训练，选择效果最佳的结果
    - 1000pg，10osd，3副本，mapping选择另存：

  <img src="..\..\photos\\image-20210513103328853.png" alt="image-20210513103328853" style="zoom:50%;" />

- 问题1：如何映射？

  - 巨大问题：每次得到状态不一致
  - 方式1：将映射关系建表
  - 方式2：分类问题？区分pg id

- 问题2：状态过多，空间爆炸；训练精度不足，需要训练很久？

  - n ^ m (1000 ^ 10 = 10 ^ 30)
  - 改善模型、改善状态定义...
  - DQN中模型选择：目前使用的MLP，结合RNN、CNN？
  - 训练参数调整，Oﬀline Training

- 问题3：如何支持增删osd

  - 减少节点m：控制action不能等于m，就可以均匀分布到其他osd；但是两次映射差距甚大...

  - 增加节点：space和action都要变，模型需要重新训练？另外重新训练和原映射差距甚大...

  - 难点：**增量迁移怎么定义模型？**

    - 想法1：将两次变化量作为reward

      - 训练难收敛，训练效果差

        <img src="..\..\photos\Load\image-20210514154000569.png" alt="image-20210514154000569" style="zoom: 50%;" />

      - 表格 + RL 再训练 

    - 想法2：多agent，每个osd相当于agent，Multi-agent Data Distribution

      <img src="../../photos/v2-6bf1f5737f9f654929fab84f73cba012_720w.jpg" alt="img" style="zoom: 50%;" /><img src="..\..\photos\image-20210513181111622.png" alt="image-20210513181111622" style="zoom: 80%;" />

      - space：weight
      - action：要 / 不要 / 滚；+ 1 / 0 / -1
      - reward：对每个agent 尽量少加 / 尽量少减；所有，标准差
      - 模型学习困难：Actor-Attention-Critic for Multi-Agent Reinforcement Learning，ICML’19
      - 数据集难以构造，有限样本

    - 想法3：动态环境下的强化学习

      - A Survey of Reinforcement Learning Algorithms for Dynamically Varying Environments，2005
      - 多模态强化学习

#### **实现2：采用原有的放置策略（hash、crush），RL作为辅助调整，即action调整mapping**

- 简单实现，mapping=crush，支持所有
  - space：各osd的容量状态，{weight0，weight1，...，weightm}
  - action：crush选出来后左右移动？
  - reward：经过调整后，标准差变化
- 问题 
  - 只能对均匀进行优化，是否影响性能？
  - 是否可以抽象成ML分类问题，RL作为调整辅助？
    - ML分类器 + RL
  - 是否可以做成一种通用框架，可以适用于各种分布算法？



#### 增量迁移设计

- 实现1：Reward: 标准差 && Map变化最少 

  ​                  <img src="..\..\photos\数据分布RL1.png" alt="数据分布RL1" style="zoom: 25%;" />                         <img src="..\..\photos\数据分布RL设计.png" alt="数据分布RL设计" style="zoom: 25%;" />

- 这需要RL做什么。。。

- 可变状态空间或者动作空间，对深度强化学习算法来说确实是一个比较棘手的问题。一旦输入空间确定，就意味着神经网络的输入节点个数确定了下来。如果在学习的过程中，输入节点增多或者减少，这对隐层乃至输出层的数据都会产生影响



- 多智能体RL
  - 

## Evaluation

## Related Work

- 分布算法（见[load balancing.md](2021-04-12-数据分布和负载均衡.md)）

- 强化学习（见[RL.md](2021-04-23-replica placement with RL.md)）

- 强化学习存储研究（见[RL.md](2021-04-23-replica placement with RL.md)）

  

#### Towards Self-Managing Cloud Storage with Reinforcement Learning

- 主题Ceph-RL，机构University of Texas at San Antonio，https://ieeexplore.ieee.org/abstract/document/8789906

- IEEE International Conference on Cloud Engineering (IC2E)，未列入ccf推荐，算云计算较好会议

- 提出：In Ceph，**Reinforcement Learning Based Adaptive Load Balancing and Data Migration**

  <img src="..\..\photos\Load\021800a034-fig-1-source-large.gif" alt="021800a034-fig-1-source-large" style="zoom: 50%;" />            <img src="..\..\photos\Load\021800a034-fig-5-source-large.gif" alt="021800a034-fig-5-source-large" style="zoom: 50%;" />



> **这朵花的香味，从音乐的音阶上来说，就是尖锐的‘La’。— 小三**