---
title: 'Replica Placement with RL'
tags:
  - Load_Balancing
---

# Learning Replica Placement with RLRP

## Abstract

## Introduce

1. 分布式/对象存储等等
2. 分布问题：负载均衡、可靠性问题（副本和故障域）、迁移扩容等等
3. 强化学习为解决分布问题带来曙光
4. 我们的方案

## Motivation

- 强化学习

- 问题建模

  - 副本放置问题: M个数据存到到N个机器上, 每个数据R个副本在不同机器上, 下图中R=2

    ​	<img src="..\..\photos\load.png" alt="load" style="zoom: 25%;" />

- 目标: 1. 每个机器上的数据尽量均匀

- 后续保证: 2. 每个机器上的主副本尽量均匀；3. 机器异构环境

- 具体到Ceph

<img src="..\..\photos\背景ceph.png" alt="load" style="zoom: 25%;" />

- 算法目标
  1. 均匀性：分布均匀
  2. 一致性：相同输入，相同输出
  3. 可靠性/可用性：副本放置，故障域隔离
  4. 可扩展性：增/删节点处理，增量迁移
  5. 其他：性能指标、分布式CAP等等

## Design

With RL

- 环境定义（park）+ 模型实现（tensorflow），基本框架：

<img src="..\..\photos\RL.png" alt="RL" style="zoom: 33%;" />

- 两种思路

  - RL作为映射算法
  - RL作为辅助，调整原有映射算法

- **实现1：RL训练出放置策略，action space即为mapping策略**

  - 简单实现，目前支持映射均匀性、副本机制、增删pg节点（n->m）

    - **一致性 、扩容暂不支持**
    - spcae：各osd的容量状态，{weight0，weight1，...，weightm，}
    - action：{osd0，osd1，...，osdm}
    - reward：各osd容量标准 / 前后标准差变化 / 极差
    - policy : 当前状态下选择的action，是训练结果
    - 模型：当前实现Q-learning和DQN
    - 实现和效果
      - 多次迭代训练，选择效果最佳的结果
      - 1000pg，10osd，3副本，mapping选择另存：

    <img src="..\..\photos\\image-20210513103328853.png" alt="image-20210513103328853" style="zoom:50%;" />

  - 问题1：如何映射？

    - 巨大问题：每次得到状态不一致
    - 方式1：将映射关系建表
    - 方式2：分类问题？区分pg id

  - 问题2：状态过多，空间爆炸；训练精度不足，需要训练很久？

    - n ^ m (1000 ^ 10 = 10 ^ 30)
    - 改善模型、改善状态定义...
    - DQN中模型选择：目前使用的MLP，结合RNN、CNN？
    - 训练参数调整，Oﬀline Training

  - 问题3：如何支持增删osd

    - 减少节点m：控制action不能等于m，就可以均匀分布到其他osd；但是两次映射差距甚大...

    - 增加节点：space和action都要变，模型需要重新训练？另外重新训练和原映射差距甚大...

    - 难点：**增量迁移怎么定义模型？**

      - 想法1：将两次变化量作为reward

        - 训练难收敛，训练效果差

          <img src="..\..\photos\Load\image-20210514154000569.png" alt="image-20210514154000569" style="zoom: 50%;" />

        - 表格 + RL 再训练 

      - 想法2：多agent，每个osd相当于agent，Multi-agent Data Distribution

        <img src="../../photos/v2-6bf1f5737f9f654929fab84f73cba012_720w.jpg" alt="img" style="zoom: 50%;" /><img src="..\..\photos\image-20210513181111622.png" alt="image-20210513181111622" style="zoom: 80%;" />

        - space：weight
        - action：要 / 不要 / 滚；+ 1 / 0 / -1
        - reward：对每个agent 尽量少加 / 尽量少减；所有，标准差
        - 模型学习困难：Actor-Attention-Critic for Multi-Agent Reinforcement Learning，ICML’19
        - 数据集难以构造，有限样本

      - 想法3：动态环境下的强化学习

        - A Survey of Reinforcement Learning Algorithms for Dynamically Varying Environments，2005
        - 多模态强化学习

- **实现2：采用原有的放置策略（hash、crush），RL作为辅助调整，即action调整mapping**

  - 简单实现，mapping=crush，支持所有
    - space：各osd的容量状态，{weight0，weight1，...，weightm}
    - action：crush选出来后左右移动？
    - reward：经过调整后，标准差变化
  - 问题 
    - 只能对均匀进行优化，是否影响性能？
    - 是否可以抽象成ML分类问题，RL作为调整辅助？
      - ML分类器 + RL
    - 是否可以做成一种通用框架，可以适用于各种分布算法？

## Evaluation

## Related Work

- 分布算法（见[load balancing.md](2021-04-12-数据分布和负载均衡.md)）
- 强化学习发展（见[RL.md](2021-04-23-replica placement with RL.md)）
- 强化学习存储研究（见[RL.md](2021-04-23-replica placement with RL.md)）

