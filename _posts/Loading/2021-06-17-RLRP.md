---
title: 'paper: RLRP'
tags:
  - Load_Balancing
---

Learning Data Placement with RLDP in Modern Distributed Storage Systems  



## Abstract

With the development and maturity of cloud technology, more and more organizations,  companies, and individuals have adopted cloud platforms. In cloud environments, the cloud  storage  service  is  one  of  the  core  services. 

我们使用在Ceph中使用RLRP替代Crush算法，

## Introduce

- （**数据放置**对分布式系统可靠性非常重要）

~~新兴的的大规模分布式存储系统面临着在数十甚至是数百数千的存储设备之间分发PB这个数量级别数据的艰巨任务~~

~~我们用数据对象指代存储系统中某一层次的数据单元，它可以理解为传统文件的分片，也可以理解为数据块组，也可以理解为面向对象存储系统中的对象。当存储系统的容量达到PB（10 5B）规模时，系统中维护的数据对象个数将达到10 9级别，包含数以万计的具有不同服务功能的存储节点，在这种规模下如何将数据对象按照存储节点的服务能力映射到各节点上成为一个很复杂的问题。~~

massive amounts of data  

随着数据的爆炸式增长，新兴的的分布式存储系统数据量增大，集群规模大，这在性能、可扩展性和可靠性方面都有巨大的挑战。其首要任务是，在数万或数十万个存储设备中分配pb级的数据。这需要设计一种高效、公平、自适应的数据分布算法。不好的数据分布算法往往会导致数据的资源使用的不平衡，系统的一部分容易成为不必要的瓶颈，导致更长的延迟、更低的系统容量和更低的可用性。

With the explosive growth of data, emerging distributed storage systems  with massive amounts of data and larger clusters are facing great challenges in performance, scalability and reliability.  The primary task is to distribute petabytes of data among tens or hundreds of thousands of storage devices.  It’s necessary to design an efficient, fair and adaptive data placement algorithm. Poor data placement algorithms often lead to imbalance in resource usage, which make part of the system becoming an unnecessary bottleneck, leading to longer delays, wasted storage space, and diminished reliability. 

现代分布式存储系统给数据分布算法带了更高的要求和挑战，1）除了数据量和存储设备的增加，对存储资源的分布均匀，同时保证时间和空间高效，更加困难。2）随着时间的推移，分布式存储中会不断地加入新设备和删除旧设备。扩容和数据迁移是非常常见的，一个可扩展的数据管理方法是必须的，能够根据存储规模的变化调整数据的布局, 基于很少数量的数据迁移重新使得数据的均匀分布。3）设备发生失效的可能性更大。 在 PB 级的文件系统中, 设备失效几乎每天都会发生, 如此频繁的失效导致严重的数据丢失[ 2] 。所以数据算法必须提供一定的容灾机制，常见的包括副本复制和纠删码等等。4）另外设备异构性，随着存储设备的不断发展，更多高性能低延迟的存储设备在使用不同节点被使用。每个节点存储能力存在差异，这包括容量、网络、IO带宽等等。 算法应该综合考虑节点异构性，合理分布使系统达到最佳性能。这些需求给数据分布算法提出了很大的挑战。

~~实现可扩展和可靠的数据管 理之关键是设计一个能够自适应存储规模变化 公平 冗余 和高可用的数据布局算法, 能够在设备间公平地分布数据, 当存储设备集合改变时迁移最少的数据量, 同时提供冗余 机制提高数据的可靠性, 以及最大化系统的可用性等~~

Modern distributed storage systems have the following characteristics, which bring higher requirements and challenges to data placement algorithm. 1) Larger data volume and larger number of nodes. It is more difficult to distribute storage resources fairly, while ensuring time and space efficiency.  2) System scalability. Expansion and data migration are very common in modern distributed storage systems. A scalable algorithm is necessary, which can adjust the placement of data  in response to changes in storage scale, and re-distribute data fairly with a small number of data migrations. 3) Higher reliability requirements. In the petabytes-level storage system, devices failures occur almost every day, and frequent failures cause serious data loss[??]. Therefore, the algorithm must provide disaster recovery capability, such as data replication and erasure codes, etc.  4)  Heterogeneous storage nodes.  With the development of storage devices, more high-performance and low-latency storage devices, such as non-volatile memory (NVM), NVMe and Optane SSD, are used in different nodes. The storage capacity of each node is different, which includes capacity, network, IO bandwidth, and so on. The algorithm should comprehensively consider the heterogeneity of nodes and reasonably distribute the nodes to achieve the best performance of the system.  These requirements pose great challenges to the data placement algorithm.  

传统算法在现代分布式存储系统表现出很大的局限性，往往顾此失彼，很难满足这些需求。最具代表性的数据分布包括：1）集中式的映射，它提供数据块和存储系统之间的映射（以表或者目录形式），用于定位数据项和复制。被许多文件系统例如GFS和HDFS等等采用。然而随着数据量增多，表或者目录会线性增长，导致存储空间开销和查找数据所需的开销会越来越大。2）以hash算法为代表的去中心化的映射方式，基于散列的数据放置方案通过提供数据和服务器之间的确定性映射，消除了维护全局映射的成本。常见的包括hash-based：线性hash【？】、可拓展hash[?]、rush、一致性hash[?]、crush[?]、random slicing等等，在分布式系统中得到广泛的应用。比如，Amazon的Dynamo系统通过虚拟节点优化一致性hash，保证系统高可扩展性和高性能。Ceph采用伪随机CRUSH算法，提供很高的灵活性和可靠性。但是它们在现代分布式系统中也有很大的缺陷，以CRUSH为例，因为副本的选择机制，数据平衡性较差且存在不可控的数据迁移。[?] 另外这些算法并没有考虑设备异构性，或者只考虑了部分差别，比如节点的容量等，这在现代存储系统中难以发挥异构节点最佳性能。

  ~~3）综合两则~~

Traditional algorithms make trade-offs on these requirements and show great limitations in modern distributed storage systems. The most representative data distribution mapping algorithms include: 1) Global mapping strategies  provide a global mapping （a table or directory）between data blocks and storage systems for locating data items and replicas, which are adopted by many distributed file systems, such as GFS[?], HDFS[?] and so on.  However, as the amount of data increases, tables or directories  grow linearly in the number of data blocks, resulting in higher storage space and data search costs.  2) Decentralized mapping represented by the Hashing. Decentralized mapping schemes for data placement eliminate the cost of maintaining global mappings by providing a compact function (hashing, pseudo-hashing, etc.) between data and servers. Hash-based schemes including Linear hash[?] , Extensible hash[?] , rush, consistency hash[?] , crush[?] and random slicing, etc. are widely used in distributed systems. For example, Amazon's Dynamo system optimizes the consistent by hash virtual nodes to achieve high scalability  and Ceph uses the pseudo-random CRUSH algorithm to provide high flexibility and reliability.  But they also have big flaws in the modern storage environment. Take CRUSH as an example, Its replica selection strategy often results in unbalanced data placement and uncontrolled data migration. In addition, these algorithms do not consider the heterogeneity of devices, or only consider some differences, such as the capacity of nodes, which makes it difficult to achieve the best performance of heterogeneous nodes in modern storage systems.

~~the storage space and the cost of finding the data will become more and more expensive~~ ~~it is difficult to meet these requirements because they tend to catch one and lose another.~~ ~~A scalable data placement method is necessary, which can adjust the layout of data according to the changes in storage scale, so that data can be stored in new devices.  Re-distribute fairly on the set and control the smaller amount of migration~~~~Many academic studies and industrial projects have tried to  propose lots of excellent data placement algorithms.~~  

~~1）被广泛使用的是（called Global mapping）~~~~2）以hash函数为代表的去中心化的映射方式，他们不维护~~~~3）是综合两种方式，~~~~但是这些研究在面对负载多变的现代分布式存储中存在各种问题，往往在 trade-off。另外由于要支持副本以及存储节点的异构性，很多算法处理不了，造成系统性能瓶颈。或者为了达到最高性能，资源往往集中于高性能的节点上，造成性能热点问题，导致。大规模的设备~~

~~the replica placement problem is a combinatorial optimization problem~~

很多研究表明，现代存储系统中带副本的数据分布问题，可以看成是组合优化问题。近年来，通过不断发展，RL在被用于解决各种系统问题，是被认为是解决组合优化问题的很好的方法。为此我们提出了RLDP，一个基于强化学习的数据分布算法，它通过结合全局映射和散列隐射，

~~公平的、冗余的、自适应的、高可用的、高效的数据分布算法，能够很好的满足现代分布式存储系统的各种需求。并且解决了~~

Many studies have shown that the data distribution problem with replicas in modern storage systems can be regarded as a combinatorial optimization problem.  In recent years, with continuous development, Reinforcement Learning (RL) is widely used to solve various system problems and provide a good way to solve combinatorial optimization problems.



## Background and Motivation

- Data Placement Algorithm

分布式系统中，数据分布算法的任务是将数据分发到各个设备中，这可以看成是一个“balls into bins” model的典型案例。后面我们用数据对象指代存储系统中某一层次的数据单元，它就是“balls into bins” model中的ball。在存储系统中，它是数据分配和访问的单元，它可以代指传统文件的分片，可以指数据块组，也可以理解为面向对象存储系统中的对象。存储系统由大量后端存储节点(称为数据节点，DNs)组成，每个后端存储节点具有一个或多个cpu、本地内存和本地连接的存储数据的设备。 它们是“balls into bins” model中的“bins”。  因此，数据分布任务就是，接受可以创建对象的前端客户端的指令，将对象分配到数据节点中。根据现代分布式存储的特点，数据放置方案可以根据以下标准进行比较:  

1）公平性:如果一个方案允许存储在数据节点上的对象的比例等于(或至少接近)它在系统总容量中所占的份额，则该方案称为公平的。公平性可以用数据节点的相对权重(对象数量除以权重)的标准差来衡量。

2）自适应性：如果一个方案在系统规模变化时，可以以最小迁移数据量达到系统重新均匀分配，则该方案可以称为自适应的。自适应性可以由系统规模变化时算法迁移的数据量与最优迁移的数据量之比值来衡量。

3) 冗余:一个方案在存储数据时采用了多个副本或擦除码，称为冗余。  

4) 一个方案应该能够考虑数据节点的异构性，考虑节点负载性能等多种因素，以实现系统最佳性能。



In  distributed storage systems, the task of the data placement algorithm is to distribute data to various devices, which can be regarded as a typical case of the "balls into bins" model. In the following we use a data object (or just object) to refer to each logical data, which is the "ball" in the "balls into bins" model.  the storage system, it is a unit of data allocation and access. It can refer to a traditional file fragment, a data block group, or an object in an object-based storage. The storage systems consisting of a large number of back-end storage nodes (called Data Nodes, DNs), each with one or more CPUs, local memory, and locally attached devices for storing data. And DN is the "bin'' in the ``balls into bins" model. Therefore, the task of the data placement strategy is to take the instructions from front-end clients and distribute objects to Data Nodes. Clients of the system can read, create, and update objects. Based on the characteristics of modern distributed storage, the data placement scheme can be compared based on the following criteria:

1) Fairness: A scheme is called fair if it allows  the fraction of objects stored at a data node is equal (or at least close) to its share of the total capacity of the system. Fairness can be measured by the standard deviation of the relative weight (the number of objects divided by weight) of the data nodes.

2） Adaptivity: A scheme is called adaptive if it can achieve fair redistribution with the near-minimum amount of migrated data in the case that there is any change in the number of objects, data nodes, or the capacities of the system.  The adaptivity can be measured by the ratio of the amount of data migrated by the scheme to the amount of data optimally migrated In theory when the system scale changes.  

3) Redundancy: A scheme can be called redundant if it adopts multiple replicas or erasure codes when storing data.  





It should be possible to schedule based on device availability adjust the location of data to maximize the performance of the system.

~~The layout algorithm makes the percentage of data on each device equal to its relative weight, so the layout algorithm is fair~~ 

~~1）均匀性，2）~~~~传统算法的局限性。在我们的测试中，我们对传统算法的典型代表（。。。）进行了测试，如表？？。~~

强化学习是机器学习的一个领域，它关注的是人工代理应该如何在特定环境中采取行动，并以累积奖励概念最大化为目标。一个基本的RL模型由agent和环境构成，并且状态、动作、策略和汇报组成其基本要素。具体的，智能体在每一步的交互中, 都会获得对于所处环境状态的观察（有可能只是一部分）, 然后决定下一步要执行的动作。环境会因为智能体对它的动作而改变, 也可能自己改变。智能体也会从环境中感知到奖励信号, 一个表明当前状态好坏的数字。智能体的目标是最大化累计奖励, 也就是回报。强化学习就是智能体通过学习来完成目标的方法。

Reinforcement Learning[?]  is an area of machine learning concerned with how artificial agents ought to take actions in a certain environment with with goal of maximizing some notion of cumulative reward.  RL problems can be formulated as a Markov decision process (MDP), which is used to describe the process of interaction between agent and environment, and  the State (S), Action (A), Reward (R) constitute its basic elements.  Specifically,  In RL, the environment is the world that the agent lives in and interacts with. At every step of interaction, the agent sees a (possibly partial) observation of the state of the world, and then decides on an action to take. The environment changes when the agent acts on it, but may also change on its own. The agent also perceives a reward signal from the environment, a number that tells it how good or bad the current world state is. The goal of the agent is to maximize its cumulative reward. The RL method are a way that the agent can learn behaviors to achieve its goal. 

总的来说RL可以分为基于价值和基于策略的方法。基于价值的方法输出所有动作的收益，并选择最高收益的策略，显然它仅限于离散的动作空间。而基于策略方法是适用于连续的动作空间，他的输出是策略而不是具体的值，可以根据策略直接选择动作。在我们的问题中，动作空间是离散的，因此the value-based method被采用。

In general, RL can be divided into two categories: value-based and policy-based method. The value-based method outputs the value or benefit (generally referred to as Q-value) of all actions and chooses the action with the highest Q-value, obviously limited to discrete action spaces. The policy-based method is applicable to the continuous action space, and its output is a policy rather than a value, and the action can be immediately outputted according to the policy. The policy-based method is sufficient because of the discrete action space in RLDP.  

Q- learning和Deep Q Network (DQN)是最经典的基于值的RL方法。Q-learning使用Q-table记录行为值 (Q value) , 每种在一定状态的行为都会有一个值 Q(s, a)，通常是状态和动作的二维表。策略就是每次根据状态选择Q表中最大Q-value的动作。

Q value的更新是根据贝尔曼方程：

其中r为从状态st移动到状态st+1时所获得的奖励，a为学习率0 < a≤1。 当g = 1时，Q函数等于奖励之和，当g = 0时，Q函数只考虑当前奖励。 当状态st+1为最终或终结状态时，算法的一集结束。 然而,q学习的  

Q-Learning and Deep Q Network (DQN) are the most classic value-based RL methods. Each action in a certain state will have a value (Q-value), which are defined as Q(s, a). Q-learning records Q-value by a Q-table, which is usually a two-dimensional table of states and actions. The policy is to choose the action, which has the maximum Q-value in the Q-table, according to the state. Q-value will be updated at each step according to the Behrman equation:  
$$
Q(s_t, a_t) \gets Q(s_t, a_t) + \alpha[r + \gamma  max_{a_{t+1}} Q(s_{t+1}, a_{t+1})-Q(s_t, a_t)]
$$
where $\alpha$ is the learning rate, 0 $\textless$ $\alpha \leq$ 1; r is the reward received when moving from the state $s_t$ to the state $s_{t+1}$; $\gamma$ is a discount factor, which favors short-term reward if close to zero and strives for a long-term high reward when approaching one.



In RLDP，采用DQN代替Q-learning，是因为Q-learning难以解决大规模分布式系统中数据放置问题中存在的状态空间大的问题。 DQN在q学习中使用神经网络而不是q表来评估q值。 输入是状态，输出是DQN中所有动作的q值。  

 Deep Q Network (DQN)  is adopted instead of Q-learning because Q-learning is hard to solve the problem of a large state space, which occurs in data placement problems in large-scale distributed systems. DQN uses neural networks rather than Q-tables in Q-Learning to evaluate Q-value, which is able to solve the problem of large state space effectively.  





- Challenges

据我们所知，我们是第一个使用强化学习来优化存储系统数据分布的算法，这将面临非常大的挑战：1）如何对问题进行建模，如何定义强化学习各要素？2）如何处理迁移问题？3）当节点数和数据量比较大的情况下，如何加速训练？4）异构环境，如何兼顾性能和均衡性？RLDP很好的解决了这些问题，为强化学习解决系统问题提供了很好的案例。

 this is the first work using reinforcement learning to optimize the data distribution of storage systems, which will face daunting challenges : 1) How to model the problem and how to define the elements of reinforcement learning? 2) How to deal with migration issues? 3) How to speed up training when the number of nodes and the amount of data are relatively large? 4) How to balance performance and fairness in a heterogeneous environment? etc. RLDP solves these problems well and provides a good case for reinforcement learning to solve system problems.  



10000 objects

100 osds



分布均匀、查找时间、副本放置、迁移最小化、异构环境

|                 | 分布均匀 | 查找时间 | 空间占用 | 副本放置 | 迁移最小化 | 异构环境 |
| --------------- | -------- | -------- | -------- | -------- | ---------- | -------- |
| table-based     |          |          |          |          |            |          |
| consistent hash |          |          |          |          |            |          |
| crush           |          |          |          |          |            |          |
| ASURA           |          |          |          |          |            |          |
| Random Slicing  |          |          |          |          |            |          |



- （**目标**）

Placement schemes for storing redundant information can be compared based on the following criteria (see also [Brinkmann et al. 2002]):

1) Capacity Efficiency and Fairness. A scheme is called capacity efficient if it allows us to store a near-maximum number of data blocks. We will see in the following that the fairness property is closely related to capacity efficiency, where fairness describes the property that the number of balls and requests received by a bin are proportional to its capacity.

2) Time Efficiency. A scheme is called time efficient if it allows a fast computation of the position of any copy of a data block without the need to refer to centralized tables. Schemes often use smaller tables that are distributed to each node that must locate blocks. 

3) Compactness. We call a scheme compact if the amount of information the scheme requires to compute the position of any copy of a data block is small (in particular, it should only depend on n—the number of bins).

4) Adaptivity. We call a scheme adaptive if it only redistributes a near-minimum amount of copies when new storage is added in order to get back into a state of fairness.  

5) 对于真实系统中，要考虑各种异构环境和因素，要兼顾性能和均衡性

- （其他分布算法的问题）

**【测试1】**，突出各个算法的问题

table-based

hash-based

1）一致性hash

2）crush

3）Random Slicing

4）。。



~~The problem, due to its generality, is studied in many other disciplines(e.g. game theory, control theory, etc). In the case of machine learning, the environment is typically formulated as a Markov Decision Process (MDP), as many reinforcement learning algorithms for this context utilize dynamic programming techniques. The main difference between reinforcement learning and the classical dynamic programming methods is that in RL do not assume knowledge of an exact mathematical model of the MDP. An advantage of RL algorithms is that they can target large MDPs where exact methods become impracticable.~~

## Design

- （基本模型）

如图所示，RLRPA   is composed of a set of virtual nodes and a set of data storage nodes， 

RLRP系统



<img src="..\..\photos\RL\RLRP.png" alt="RL" style="zoom: 18%;" />

- Environment

Our virtual nodes have the same concept of virtual nodes in Dynamo and partitions in OpenStack-Swift which is an abstract layer for managing all system data into smaller parts, i.e., a set of objects, as it is shown in the hash mapping layer of Figure 6. Each data object on the system is mapped to a virtual node through the consistent hash function mapping. A hash function applies the identification of a data object to calculate the modulo operation using the total number of virtual nodes, defining then which virtual node the object belongs to. The number of objects in every virtual node is balanced due to the hash function of the hash mapping layer that outputs hashed values uniformly distributed. The hash function responsible for mapping data objects to the virtual node is set up only once and remains the same during the entire system operation. At deployment, before system start-up, the system administrator sets the total number of virtual nodes to a large value and never changes it; otherwise, it would break the property of  the consistent hashing technique by creating the side-effect of huge data movements. 

A virtual node can be replicated multiple times on different storage nodes, for example, Virtual Node 2 is replicated to Storage Nodes 1 and 3 in Figure 6. A Replica Placement Scheme (RPS) is responsible for defining the mapping of virtual node replicas to storage nodes. It specifies the replication factor, which happens to be 2 in our example meaning that each virtual node has two copies in our storage system and the placement of every virtual node replica as shown in Figure 6. By modifying the replica placement scheme, the storage system can dynamically manage data through operations of replica creation, migration and deletion. In this work, the RPS is modified in order to achieve load balancing of Get operations.  

- Common Interface

  - node can be replicated

- Memory Pool

- RL Agent

  ​		<img src="..\..\photos\RL\RL_NN.png" alt="load" style="zoom: 25%;" />			<img src="C:\lukai1\桌面\商汤实习\Blog\emperorlu.github.io\photos\RL\Migration.png" alt="Migration" style="zoom: 25%;" />             

  - Placement Agent
  - Migration Agent
  
- （迁移模型）

The disk structure of each Sub-tree is based on SStables like RocksDB. But RLRP abandons the design of index block in SStable, and uses efficient learned index to build a learned index block to improve the SStable read performance. The learned index block includes two modules: String Process and Learned Index. String Process is mainly responsible for efficiently converting strings into integer numbers for model training. For Learned Index, we use a two-layer linear model of RMI[26], which is sufficient to handle most workloads[26, 45]. Different from the traditional binary search, the learned index block directly predicts the block where it is located through the model, which is faster than the binary search. More importantly, the learned index block is much smaller than the traditional index block, and can be cached more in memory, which is more conducive to read performance. 

As shown in the left of Figure 6, RLRP uses the high-performance SPDK interface to manage the NVMe SSDs, avoiding the system overhead caused by the Linux I/O path. In addition, RLRP adopts  asynchronous IO for read process and provides asynchronous interfaces for the upper and lower processes. Aynchronous IO makes it possible to reduce the synchronization control overhead caused by competing I/O resources, and to coordinate the optimization of front-end and background processing speed. Read process. As shown in Figure 6, the read process is as follows: the upper-level program calls the asynchronous interface to access TridentKV. RLRP firstly searches the Memtable, Immutable Memtable and Partition Scheduler in turn in the memory. If not found, RLRP finds in the corresponding Sub-tree according to the map and searchs the SStable files. For read files, firstly RLRP read the bloom filter to determine whether it exists. Then quickly locate its data block through the learned index block, load it into the memory and find target key. 



=INDIRECT("a"&ROW(A1)*3)&""



（训练优化）

The disk structure of ecach sub-tree is based on SStables like Rocksdb. But RPRL abandons the design of learned index in SStable and use efficient learned index to build a learned index block to improve the SStable read performance. The learned index block includes two modules: String process and Lerarned index Block. String Process is mainlu readsponsible for efficentne converting string into ineteger numbers for model training. For Learned Index, we use a two-layer linear model of RMI, which is sufficient to handle most workloads. Different form the tradition binary search, the learned index block directly predicts the block where it is located through the model, which is faster than the binary search. More importantly, the learned index block is much smaller than the traditional index block, and can be cached more in memory, which is more conducive to read performance. As shown in the left  of Figure 7, RLRP uses the high-performance SPDK interface to manage the NVMe SSDs, avoiding the system overhead caused by the Linux I/O path. In addition, RLRP adopts asynchronous IO for read process and provides asynchronous interfaces for the upper and lower processes. Asynchronous IO makes it possible to reduce the synchronization control overhead caused by competing IO resources, and to coordinate the optimization of front-end and backgroung processing speed. 

Read process. As shown in Figure 8, the read process is as follows: the upper-level program calls the asynchronous interface to access TridentKV. RLRP firstly searches the Memtable, Immutable Memtable and Partition Scheduler in turn in the memory. If not found, RLRP finds in the corresponding sub-tree according to the map and searches the Sstable files. Foer read files, firstly RLRP read the bloom filter to determine whether it exists. Then  quickly locate its data block throught the learned index block, load it into the memory and find target key. In the process of replica placement, the basic principle when placing a repliuca need to be followed. Under the premise of not violating the basic principle of replica placement, the system can maintain high availability while achieving the balance of replica placement distribution as much as possible. The replica placement rules defined in this paper are as follows:  

1) Different replicas for the same data block should be placed on different data nodes. We need to ensure that the availability of the replica is not affected. The default replica placement strategy considers the impact of the node failure or the rack failure on the overall system availability. If all replicas for a data block are placed on a node, once the node fails, all replica will be lost and unrecoverable. Therefore, we should avoid placing the replicas of the same data block on the same node. 

2) Data replicas should be placed on a node with high availability. A node with high availabity is the node with fewer failures and less load in the cluster system over a period.  After the nodes fails, the data block can obtain relicas from other nodes and take over the failed noed tasks in order to ensure uninterrupted service of the cluster system. If a replica is placed on a node with low availability, the replica tends to be unavailabile as the node fails which result in losing the meaning of the data replica. Therefore, the replica should be placed on a node with high acailability.

<img src="..\..\photos\RL\train_RL.png" alt="load" style="zoom: 25%;" />

(3) The data block replicas should be evenly placed throughout the cluster system. The global placement of the data replica determines the amount of overhead for the node to access the nearest replica of the data block. If the data block replicas are placed in a centralized manner rather than evenly placement in the cluster system, although the overhead of some nodes is small in accessing the nearest replica of the data block, most nodes have a high cost of accessing the nearest replica. Therefore, the data block replicas should be evenly placed throughout the cluster  ceph.  After the nodes fails, the data block can obtain replicas from other nodes and take over the failed nodes tasks in order to ensure uninterrupted service of the cluster system. If a replica is place on a node with low avaibility，

- （异构场景）

  <img src="..\..\photos\RL\Attention.png" alt="Attention" style="zoom:18%;" />

​      The dynamic multi-objective optimized replica placement and migration strategies for SaaS applications in edge cloud are proposed to deal with the problems mentioned above. The multiobjective optimized replica placement problem is solved according to the fast non-dominated sorting genetic algorithm. With following the defined replica placement rule, multiple factors including the file unavailability, node load and network transmission cost are considered comprehensively. Then, the optimal replica placement target selection strategy can be obtained according to the spatial congestion degree. Finally, the consistency management of replicas is performed by constructing the reliability record table to ensure the availability of the data and the reliability of replica placement. Besides, the replica migration model for access hotspots is proposed in order to deal with the file access hotspots problem caused by burst requests. The method is mainly divided into three parts, including obtaining the load of the data node, the hot file selection strategy and the dynamic replica migration. The data nodes are divided into overload nodes, equalization nodes and low load nodes according to the load status threshold. The number of required replicas can be obtained according to the hotspot file selection. The pairing migration relationship from the source node to target node is obtained. The main contributions are shown as follows 

尽管B-tree在读密集场景发挥更加出色，但是在现代KV存储中，大数据量和高速存储设备的，LSM-tree还是应用的更加广泛

- In ceph

  <img src="..\..\photos\RL\RLRP-Ceph.png" alt="RL" style="zoom: 20%;" />







## Implementation  

- （In ceph）
- 

## Evaluation

- 测试平台：Cloudsim、COSBench、fio/rados benchmark

  https://github.com/intel-cloud/cosbench

- Ceph

- 真实数据

## Related Work





## 讨论

- **2021.7.6**
- 通用的数据分布的问题基本没什么研究的，问题不够新颖，应该聚焦于具体的未提出未发觉的问题上
- 投稿Transactions on Storage，A类期刊，七月底完成一版

1. **问题不明确**：背景中需要增加其他分布方案的问题？

   - 类似于 Random Slicing: Efficient and Scalable Data Placement for Large-scale Storage Systems  

   <img src="..\..\photos\RL\image-20210707141805636.png" alt="image-20210707141805636" style="zoom:50%;" />

2. 使用RL的**动机不纯**？
   - 将分布问题建模，动态分布问题可以看作组合优化问题，RL在解决组合优化问题中发挥出色

3. 迁移方案太过简单，很多因素需要考虑？
4. 方案中增加公式补充？
5. 优化表述不清楚，要结合图详细解释？
6. 测试规模小，osd目前测试到80？至少要扩到100，1000：训练极其慢？
7. 异构考虑较为简单，但是很多动态的不用考虑，应该考虑异构中静态元素。动态元素应当负载均衡考虑？
8. 异构测试效果如何对比说明，需要增加别的方案的测试结果？
9. 测试太少，增加对比测试目标：一致性hash，table-based，crush等等
10. 增加真实系统，真实数据的对比测试
11. **测试补充**

- 增加其他分布算法的测试
  - consistent hash：crush：random slicing：table-based：
- 1000 osd的模型训练
  - 放低要求，加速训练
- 封装模型到ceph，真实数据对比测试

