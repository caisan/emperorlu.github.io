---
title: '数据分布和负载均衡'
tags:
  - Load_Balancing
---

本文通过分析Ceph Crush算法的问题，总结数据分布和负载均衡的相关算法和策略，思考对Crush算法的优化途径。

- Ceph-Crush 问题
  - 问题1：不可控迁移，迁移量是理论下限的h倍（h为分层结构的层数）
  - 问题2：不够均衡，CRUSH输入的样本容量不够、副本机制的缺陷
  - 问题3：选择存储节点时，仅以节点存储容量为唯一选择条件，并没有考虑其他因素（到网络和节点
    的负载状况等等）
- 相关优化In Ceph
  - 算法优化
    - MapX（fast'20）增加时间戳，开销极大，在对象存储中不可行
    - 降低迁移任务的优先级，治标不治本，只是推后了迁移，并未解决迁移带来的问题
    - 使用cluster device flags 选择性标记failed OSDs，减少数据传输
    - 选择节点考虑负载、异构环境、网络等等情况
  - 事后补救
    - balancer（Luminous版本之后，并不完善）调整映射关系，目前只支持 compat weight-set 和upmap
    - 测试显示，打开balancer模块，大约1分钟会触发一次，几分钟基本就调整完了，调整的结果非常好
    - 并不持久化，扩容或者重启OSD会重新计算分布，之前的映射失效
    - 手动调整（reweight、批量调整、upmap等）
- 数据分布&&负载均衡
  - 评判指标
    - 负载均衡：节点磁盘容量，数据访问（read/write）负载等均匀分布
    - 可扩展性：能否友好的应对增/删节点，以及日益增长的规模
    - 可靠性：数据分片需要多个副本
    - 可用性：副本放置需要考虑故障域隔离，保证更好的可用性
    - 一致性、性能指标、分布式CAP等等
  - 静态处理
    - load balancerr
    - range-based: region, shard, tablet, Random Slicing（ACM TOS'14）
    - hash-based: consistent-hashing, Crush, hash虚拟桶
    - consistent-hashing（STOC'97）
      - hash环（SOSP'07）
      - Jump Consistent Hash, Google
      - Maglev Consistent Hash（Google, NSDI'16）
    - Crush: RUSHP, RUSHR, RUSHT
  - 动态处理: round-robin, least connections, the power of two random choices
  - Load Balancing with cache
    - small cache, big effect, SoCC'11
    - switchKV, NSDI'16
    - NetCache, SOSP'17
    - DistCache, FAST'19 Best Paper
- 思考
  - 算法优化：应该尽量保留Crush算法，添加一些辅助手段
    - 类似于MapX：能否有别的方式来区分新旧对象？
  - 策略优化：选择节点有必要考量其他因素？用什么策略选择？
    - 两次hash、with cache能否借鉴？
  - 工具优化：balancer模块探究
    - balancer持久化（怎么和crush算法相结合？）
    - balancer是否足够优秀，自动调整策略是否有优化空间？
    - 大规模对象下表现如何，打开该模块是否影响性能？
    - 主pg分布调整