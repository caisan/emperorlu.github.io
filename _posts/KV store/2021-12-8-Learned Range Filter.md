# Learned Range Filter

> 玉可碎而不可改其白，竹可焚而不可毁其节 — 老关

- LeRF: An Efficient Learned Range Filter for Key-Value Stores
- AegisKV:  A Range-query Optimized LSM-tree Based KV Store via XX
- 投稿目标
  - filter单独投一篇：偏机器学习投NeurIPS  / 偏kv投系统会议
  - 范围优化kv：引用上篇；删除优化；异步scan

## 相关文章

- filter特征 
  - 存在性索引：0/1分类问题 
  - allow false positives， but will not give false negative  （有不一定有，没有一定没有）
  - tradeoff：空间与精度

- bloom filter

  - a set S ={x1，x2，xn} of **n** keys. It consists of an array of **m** bits and uses **k** independent hash functions
    {h1, h2, hk} with the range of each hi being integer values between 0 and m - 1

  - FPR(false positive rate)
    $$
    FPR = (1-(1-1/m)^{kn})^k
    $$

- bloom filter的问题

  - 对数据不感知，空间开销大，误报率高：Learned Filter
  - 不支持范围查询：SuRF，Rosetta
  - 性能低，开销大：Chucky
    - 之前的SSD/HDD等访问延迟较大，Bloom filter引起的开销比重较小，可以忽略不计
    - NVMe SSD性能提升，与DRAM性能差距进一步缩小，由于在每层都要维护Bloom filter，会引起比较大的查询延迟，引起LSM-Tree中Bloom filter成为新的瓶颈之一  

- 相关文章 

  - Range Filter for KV Stores
  - Learned Filter

### 论文1-1  SuRF: Practical Range Query Filtering with Fast Succinct Tries

- Sigmod‘18 Best Paper，CMU Andrew Pavlo团队

- 提出了一种新的数据结构 SuRF (Succinct Range Filter)，是基于 FST (Fast Succinct  Tries) 构建的一种能够支持点查询和范围查询的数据结构。FST 采用 LOUDS-DS 编码字典树， 该字典树上层节点较少， 由访问频繁的热数据组成; 下层包含大部分的节点， 由冷数据组成。LOUDS-DS 采用层内有序的布局， 利用这个特点， LOUDS-DS 能够有效地支持范围查询

  <img src="..\..\photos\Scan\image-20211209111846284.png" alt="image-20211209111846284" style="zoom: 50%;" />                                <img src="..\..\photos\Scan\image-20211209111550841.png" alt="image-20211209111550841" style="zoom: 50%;" />           

- Full Tire 能够准确查找每一条数据， 但是会占用大量的内存资源。为了平衡过滤器的误判率和所需的内存资源，SuRF 采用了剪枝字典树， 并提出了不同的优化方案：

  - SuRF-Base 只存储能够识别每个主键的最小长度的主键前缀， 但是这种方法对于主键前缀相似度较高的场景会带来较大的误判率。例如， 实验表明，对于邮件地址作为主键的数据会带来将近 25% 的误判率
  - 为了降低 SuRF-Base 的误判率， SuRF-Hash为每个主键增加了一些散列位， 查询时通过哈希函数映射到对应位检查目标数据是否存在。实验表明，SuRF-Hash 只需 2 ~ 4 个散列位就可将误判率降低至 1%。虽然 SuRF-Hash 能够有效地减低误判率，但是并不能提高范围查询的性能
  - 与 SuRF-Hash 不同的是， SuRF-Real 在每个主键前缀的后面存储n 个主键位增加主键的区分度， 在降低误判率的同时提升了点查询和范围查询的性能， 但是由于有些主键的前缀区别较小， SuRF-Real 的误判率不如 SuRF-Hash低
  - SuRF-Mixed 结合了 SuRF-Hash 和SuRF-Real 的优点， 能够有效地支持点查询和范围查询 

- 测试 

  - 代码：https://github.com/efcient/SuRF  

  - 指标：false positive rate (FPR), performance, and space

  - 负载：YCSB，email地址数据

    - The datasets are 100M 64-bit random integer keys and 25M email keys
    - We test two representative key types: 64-bit random integers generated by YCSB and email addresses (host reversed, e.g., “com.domain@foo”) drawn from a real-world dataset (average length = 22 bytes, max length = 129 bytes).

    <img src="..\..\photos\Scan\image-20211209162719477.png" alt="image-20211209162719477" style="zoom:33%;" />                             <img src="..\..\photos\Scan\image-20211209162310825.png" alt="image-20211209162310825" style="zoom: 33%;" />  

  - 应用：Rocksdb测试，YCSB负载

    - 操作：We first warm the cache with 1M uniformly-distributed point queries to existing keys so that every SSTable is touched ∼ 1000 times and the block indexes and filters are cached. After the warm-up, both RocksDB’s block cache and the OS page cache are full. We then execute 50K application queries, recording the end-to-end throughput and I/O counts

- **摒弃Excel！**

### 论文1-2  Rosetta: A Robust Space-Time Optimized Range Filter for Key-Value Stores 

- SIGMOD’20，哈佛大学[Stratos Idreos](https://stratos.seas.harvard.edu/)团队，Monkey，Dosteovsky，，，
- Rosetta：优化SuRF的范围查询能力 
  - 问题1：Short and Medium Range Queries are Significantly Sub-Optimal
  - 问题2：Lack of Support for Workloads With Key Query Correlation or Skew

- 设计：使用一组分层排列的布隆过滤器对每个主键的二进制前缀建立索引，然后将每个范围查询转换为多次布隆过滤操作

  <img src="..\..\photos\Scan\image-20211209110441425.png" alt="image-20211209110441425" style="zoom: 40%;" /><img src="..\..\photos\Scan\image-20211209110402730.png" alt="image-20211209110402730" style="zoom:50%;" />

- 测试

  - 负载1：YCSB key-value workloads that are variations of Workload E  
  - 指标：latency、CPU cost、内存占用、FPR

  <img src="..\..\photos\Scan\image-20211209160614974.png" alt="image-20211209160614974" style="zoom: 33%;" />                   <img src="..\..\photos\Scan\image-20211209161036560.png" alt="image-20211209161036560" style="zoom: 33%;" />

  - 负载2：多种真实负载，变长字符串数据集 [Wikipedia Extraction](https://aws.amazon.com/de/datasets/wikipedia-extraction-wex/)
    - We use a variable-length string data set, Wikipedia Extraction (WEX)9 comprising of a processed dump (of size
      6M) of English language in Wikipedia  
    - We generate 50 million (50 × 106) keys each of size 64 bits (and 512 byte values) using both uniform, and normal distributions.   

### 论文1-3  Chucky: A Succinct Cuckoo Filter for LSM-Tree

- SIGMOD’21，哈佛Stratos Idreos团队
- 关注点：高性能介质的使用，Bloom filter引起的开销不容忽视
- 设计：Succinct + Cuckoo Filter 
  - Chucky提出用单个Succinct Cuckoo Filter替代LSM-Tree中的多个Bloom filter，可以有效减少查找引起的开销

<img src="..\..\photos\Scan\image-20211209154820317.png" alt="image-20211209154820317" style="zoom:50%;" />             <img src="..\..\photos\Scan\image-20211209161405428.png" alt="image-20211209161405428" style="zoom: 33%;" />

- 测试：负载YCSB
- 指标：Memory I/O Scalability、FPR Scalability、Data in Storage vs. Memory、End-to-End Write Cost

### 论文2-1 A Model for Learned Bloom Filters, and Optimizing by Sandwiching

- NeurIPS 2018，一个作者，哈佛 Michael Mitzenmacher

- 原始Learned Bloom Filter

  <img src="..\..\photos\Scan\image-20211203110726171.png" alt="image-20211203110726171" style="zoom: 50%;" />

  - We then train a model with 

    <img src="..\..\photos\Scan\image-20211209194848919.png" alt="image-20211209194848919" style="zoom:50%;" />

  - that is, they suggest using a neural network on this binary classification task to produce a probability, based on minimizing the log loss function  

    <img src="..\..\photos\Scan\image-20211209194746053.png" alt="image-20211209194746053" style="zoom: 50%;" />

  - 定义

  <img src="..\..\photos\Scan\image-20211209195114883.png" alt="image-20211209195114883" style="zoom:50%;" />

  - FPR的评估

    - 与标准的布隆过滤器不同，其高度依赖于查询集，并且没有独立于查询进行很好的定义

    <img src="..\..\photos\Scan\image-20211209195715822.png" alt="image-20211209195715822" style="zoom:50%;" />
  
    - 虽然F(B)本身是一个随机变量，但FPR很集中它的期望，这只取决于过滤器|B|的大小和错误否定的数量从必须存储在过滤器中的K，这取决于f：
  
    <img src="..\..\photos\Scan\image-20211210142227258.png" alt="image-20211210142227258" style="zoom:50%;" />
  
  - 关于non-key

    - An assumption in this framework is that the **training sample distribution needs to match or be close to the test distribution of non-keys.** For many applications, past workloads or historical data can be used to get an appropriate non-key sample.  
    - 训练样本分布需要匹配或接近non-key的测试分布，对于许多应用程序，可以使用过去的工作负载或历史数据来获得适当的non-key示例 
    - 证明：Given sufficient data, we can determine an empirical false positive rate on a test set, and use that
      to predict future behavior. Under the assumption that the test set has the same distribution as future
      queries, standard Chernoff bounds provide that the empirical false positive rate will be close to the
      false positive rate on future queries, as both will be concentrated around the expectation. In many
      learning theory settings, this empirical false positive rate appears to be referred to as simply the false
      positive rate; we emphasize that false positive rate, as we have explained above, typically means
      something different in the Bloom filter literature
    - **ing**

<img src="..\..\photos\Scan\image-20211209150106669.png" alt="image-20211209150106669" style="zoom: 50%;" />

- 除了使用一个后置布隆过滤器外，三明治（Sandwiching）结构还使用了一个前置布隆过滤器
  - 由于后置布隆过滤器的大小与通过RNN模型的假阴性元素数量呈正相关，所以通过使用前置布隆过滤器消除更多的假阴性, 能够降低后置布隆过滤器的空间代价
  - 三明治结构的另一个优点是它与Kraska等人提出的学习布隆过滤器结构相比具有更强的鲁棒性。如果用于学习布隆过滤器的训练集和测试集具有不同的数据分布，那么RNN模型的FNR可能远远大于预期。增加一个前置布隆过滤器能够缓解这个问题，因为它预先过滤了一部分假阴性元素

### 论文2-2 Adaptive Learned Bloom Filter (Ada-BF) 

- NeurIPS 2020

- key分布如何求得？non-key的选取？

  <img src="..\..\photos\Scan\image-20211203145902907.png" alt="image-20211203145902907" style="zoom:67%;" />

### 论文2-3 The Case for Learned Index Structures && Partition Learned Bloom Filter

-  Tim Kraska，Jeﬀrey Dean 

  <img src="..\..\photos\image-20200929150112343.png" alt="image-20211209154031693" style="zoom:80%;" /><img src="..\..\photos\Scan\image-20211203112429236.png" alt="image-20211203112429236" style="zoom: 50%;" />

### 其他论文

- Hash Adaptive Bloom Filter， ICDE‘21

- Compressing (Multidimensional) Learned Bloom Filters，NeurIPS workshop 2020
- Meta-Learning Neural Bloom Filters，ICML’19
- Learned FBF: Learning-Based Functional Bloom Filter for Key–Value Storage，TOC'21  

### 问题总结

- Range Filter
  - Rosetta：无法避免数据的探查与探查结果的合并开销，不适合长范围查询场景
  - Surf：构建新结构，插入新数据时索引重构开销较大，Succinct 结构性能低下
  - Chucky：不支持range query 
- Learned Filter
  - 优势：感知数据模式，精度高，体积小 
  - 劣势：不支持range query，模型精度随数据不定，不支持动态插入和更新



## 设计与测试

- Learned + Range Filter两则结合

### Filter的设计

<img src="..\..\photos\Scan\image-20211203122448516.png" alt="image-20211203122448516" style="zoom:80%;" />            <img src="..\..\photos\Scan\Scan_filter.png" alt="Scan_filter" style="zoom: 25%;" />                  

- Learned Model： 二分类问题
  - f(x)的选择：**RMI**、Lr、Plr、SVM、CART、CNN、RNN
- 理论证明：FPR的评估？
- LRF设计1：（**算法问题**）求f(x)在范围内的最大值
  - 合不合理，可不可求：需要数学公式推导/证明

- ~~LRF设计2：（**插值问题**）绘制Key-Score 的映射，判断范围内最高score是否大于t；例子: (K1， k2)~~
  
  - ~~样本选择：正向样本：keys in the SStable；负向样本：non-keys的生成？~~
  
  - ~~区域最大值（极大值）：求导~~ 
  
  - Spline插值 & 多项式回归
  
    <img src="..\..\photos\paper\spline.png" alt="spline" style="zoom: 50%;" /><img src="..\..\photos\paper\test.png" alt="test" style="zoom:50%;" />
- 其他方法？
  - Kernel Density Estimation(KDE)拟合

### In KV Store

- 出发点
  
  - 基于NVMe SSD，原来bloom filter成为瓶颈
  - 不支持范围查询
  - 数据感知
  
- 设计：每个SStable配备，只读数据符合Learned index需求

- 模型和string处理， 能否结合partition learned bloom filter / Rocksdb partition bloom filtre？

  - Rocksdb partition bloom filter：full filter存储方式，可以把filter block分片为多个更小的blocks，以降低block cache的压力

  <img src="..\..\photos\Scan\string code.png" alt="string code" style="zoom: 33%;" />

- Range Filter问题：合并开销优化， 长范围查询优化

  

### 测试

- LBF的实现，尝试Lr、SVM、CNN、RNN (LSTM、GRU)、RMI

  - URL数据，CNN，模型25M，精度0.972

    <img src="..\..\photos\paper\image-20211231132349722.png" alt="image-20211231132349722" style="zoom:50%;" /><img src="..\..\photos\paper\image-20211228182703279.png" alt="image-20211228182703279" style="zoom: 50%;" />

  - DB_bench数据

    - X：0~10000，y~[0,1]，五千个0和五千个1
    - lr：0.453；SVM：0.562；RF：0.503；CNN：0.693；LSTM：0.706；双向LSTM：0.834
    - 双层双向LSTM，模型5.4M，精度0.997

    <img src="..\..\photos\paper\image-20211231132318193.png" alt="image-20211231132318193" style="zoom:50%;" />

### 求极值

- 方法1：scipy.optimize.minimize

  <img src="..\..\photos\paper\image-20211228183244222.png" alt="image-20211228183244222" style="zoom:50%;" /><img src="..\..\photos\paper\image-20211228182840354.png" alt="image-20211228182840354" style="zoom: 50%;" /><img src="..\..\photos\paper\image-20211231160227827.png" alt="image-20211231160227827" style="zoom: 50%;" />

  

- 方法2：梯度下降法

- 方法3：变成训练过程

  - 神经网络训练过程就是求最值，loss最小
  - 输入（x，y，z），参数（a，b，c），模型 ax + by + cz
  - 训练过程：寻找使得 loss = |pre - y|最小的（a，b，c）
  - 现在的情况：模型（a，b，c）是固定的，寻找使得 pre 最小的 （x，y，z）

 



- 原论文代码测试
  - 模型：GRU
  - 数据集：网址数据

```python
1. Bloom Filter
Bits needed 14293028
Hash functions needed 6
Tast False positives 0.010326521200924817

2. GRU
Params needed 2545
Bloom filter bits needed 7308941
Total bits needed 7311486
Test False positive rate:  0.010351505356869193

```

<img src="..\..\photos\Scan\image-20211209152110027.png" alt="image-20211209152110027" style="zoom: 50%;" />

### Next

- Range Filter
  - 理论支持？
  - non-key数据？
  - Range Filter如何实现？
- 测试
  - LBF其他模型测试，单独用lr精度过低
  - LBF用于Rocksdb + 其他数据集测试







