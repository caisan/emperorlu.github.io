#  Conclusions and future work

> 天上冷飕飕，地上滚绣球；有陷是包子，没陷是窝头 — 郭儿
>

- 目标1：ATC‘ 22，1月6/13日
- 目标2：FAST’ 22，7月24日

## 工作总结

### 1.  问题总结

- 文档总结 《[IO瓶颈分析](https://confluence.sensetime.com/pages/viewpage.action?pageId=236105682)》

- 出发点：Ceph并不能发挥NVMe SSD的性能

  <img src="..\..\photos\paper\sec1_ceph_problem.png" alt="sec1_ceph_problem" style="zoom:50%;" />     <img src="..\..\photos\paper\image-20211013105323650.png" alt="image-20211013105323650" style="zoom: 80%;" />   

- 性能分析

  - iostat：主要差距在于平均队列长度，最好的结果也只有4左右，这说明对下层I/O压力还是不足

  <img src="..\..\photos\paper\worddava2e2a500d9d945d8ee2b7613292fccff.png" alt="worddava2e2a500d9d945d8ee2b7613292fccff" style="zoom: 50%;" />   <img src="..\..\photos\paper\latency.png" alt="latency" style="zoom: 67%;" />

  - 火焰图，主要占用CPU时间有以下两部分:

    - **find_object_context** ：获取对象上下文，主要是对象的元数据，会从Cache获取RocksDB中读取
    - **execute_ctx**：从BlueStore中读取数据
    - 整个处理流程中，上下文切换主要发生在从磁盘中读数据，PG锁竞争，一些cond_wait

  - Ceph-Blk+lttng分析io时间

    - 对读过程所有io进行时间统计，总运行时间平均耗时2.54ms，其中osd部分1.08ms (占总的42%)，msg部分占29%
    - 进出队列时间0.4ms (占osd37%，占总的16%)
    - find_object_context有0.31ms （28.7%，12%）
    - decode有0.01ms、complete_read_ctx有0.06ms、execute_ctx平均耗时0.32ms （30%，12.5%）
    - 从trace分析，主要耗时在**网络模块**、osd中进出队列部分、**获取对象上下文**和**从bluestore中读取数据**等

  - 分段测试：网络、Bluestore、RocksDB

    - 网络：足够的压力可以是3个Worker线程满载，并且有1949.3MB/s，暂时应该不会成为主要瓶颈
    - Bluestore：不同对象大小和线程下测试，读性能上限大概是1.1G/s

  - RocksDB分析

    - 读流程分成8个过程，通过db_bench+perf分析每个过程的延迟
    - 两个大头：1. 高**读盘延迟** DBLoad 50.58%；2. 内存查找中**文件索引** IBSeek & DBSeek 15.31%

    <img src="..\..\photos\paper\image-20211013110816357.png" alt="image-20211013110816357" style="zoom: 80%;" /><img src="..\..\photos\paper\image-20211013110933994.png" alt="image-20211013110933994" style="zoom: 80%;" />                  

- 数据分布-CRUSH问题

  - 参考文档《[数据分布优化总结](https://confluence.sensetime.com/pages/viewpage.action?pageId=270923169)》

  <img src="..\..\photos\paper\image-20211013112831720.png" alt="image-20211013112831720" style="zoom:67%;" />

  - 问题1：**不可控迁移**，迁移量是理论下限的h倍（h为分层结构的层数）

  - 问题2：**不够均衡**，CRUSH 输入的样本容量不够、副本机制的缺陷

  - 问题3：选择存储节点时，仅以节点存储容量为唯一选择条件，并没有考虑其他因素（到网络和节点 的负载状况等等）

    

### 2. 工作总结

- Ceph优化：Bluefs+RocksDB+异步读+索引优化+数据分布...

  <img src="..\..\photos\paper\存储优化.png" alt="存储优化" style="zoom: 25%;" />

- RocksDB优化—**TridentKV**

  - 异步优化（+SPDK）降低读盘延迟；Learned Index加速文件索引

  - 分区优化删除问题

    <img src="..\..\photos\paper\index.png" alt="index" style="zoom: 25%;" /><img src="..\..\photos\paper\pg_partition3.png" alt="pg_partition3" style="zoom:18%;" />

- 数据分布优化

  - 结合表可控分布；结合强化学习 **RLRP**

    <img src="..\..\photos\paper\pg.png" alt="pg" style="zoom:50%;" /><img src="..\..\photos\paper\RLRP2.png" alt="RLRP2" style="zoom: 15%;" />



## 论文总结

- key-value store 2021年就有 4,490 条记录

![Ceph优化](..\..\photos\Ceph优化.png)

- 以上图为基础，补充最新的论文

### 主题1：元数据优化—高性能KV 

- **索引结构**

  - 结合场景和介质特性等等，优化索引
  - LSM-tree（L）, B+tree（B）, Tire-tree（T）, Bε-tree（E）, Bw-Tree（W），Hash（H），SkipList（S）
  - SLM-DB：**LB**；LSM-trie ：**LT**；SplinterDB：**LE**；ArkDB：**LW**；UniKV：**LH**；Skiptree：**LS**
  - 参考文档《[KV索引综述]()》(doing)

- **结合新介质 && 混合设备**：SpanDB（Fast‘21），KVIMR（ATC’21），Facebook（ATC‘21）

  - 参考文档《[NVMKV综述](../../files/NVMKV综述.pdf)》

- **Far memory && DSM**：DrTM+R（EuroSys’16），DrTM+H（OSDI’18），DrTM+N（ACT‘21），Xstore（OSDI’20）

  - 参考文档《[分布式存储](../../files/分布式存储.pdf)》

- **LSM-tree & RocksDB**

  - LSM-based storage techniques: a survey （VLDP 20）

  ![20210217213132161](..\..\photos\paper\20210217213132161.png)

  ![image-20211014175206648](..\..\photos\paper\image-20211014175206648.png)

  - 参考文档《[键值存储优化论文综述](../../files/键值存储优化论文综述.pdf)》
  - 读写放大
    - KV分离：Wisckey，HashKV，UniKV，DiffKV（ATC‘21）
    - 允许部分键值覆盖：PebblesDB，LSM-tire，SifrDB
    - 增大内存：Skiptree，VT-Tree，FloDB
  - Compaction：TRIAD，LDC，LSbM，SILK，X-Engine（FPGA），
    - Constructing and Analyzing the LSM Compaction Design Space，VLDB’21
    - 把compaction问题抽象建模（设计空间），为如何和何时compaction提供指导
  - 尾延迟（compaction导致写停顿）：SILK，bLSM，KVell，MatrixKV，CruiseDB，RocksDB（写节流算法）
  - 空间优化：SuRF，SlimDB，RocksDB
  - **读优化**
    - 参考文档《[RocksDB读优化总结](..\KV store\2020-09-25-RocksDB读优化研究.md)》
    - 索引优化
      - X-Engine，SLM-DB，Kvell，**Bourbon**&&Google
    - cache
      - Ac-key：多种cache自适性选择
      - LSbM-Tree，Leaper（VLDP'21）：compaction之后读cache命中率下降严重
    - filter
      - BLSM：第一次使用bloom过滤器
      - ElasticBF，monkey：动态调整filter大小
      - SuRF：支持范围查询；
      - Rosetta，Chucky（Sigmod’21）
  - **删除优化**
    - 参考文档《[RocksDB删除问题总结](..\KV store\2020-10-03-Delete in Rocksdb)》
    - Lethe：定期 && 分区
    - RocksDB / MyRocks：SingleDelete和定期触发compaction
  - 范围查询
    - RocksDB：前缀filter
    - REMIX（Fast’21）：分析LSM基于迭代器范围查询过程，通过记录键值映射和游标加速范围查询
  - CPU
    - Kvell：share-nothing； 
    - Kreon：系统调用开销大，优化内核
    - uDepot：异步架构
  - 自动调优：Monkey，Dosteovsky，ElasticBF
  - 性能测试、自动调参、特定负载和场景等等
    - Toward a Beter Understanding and Evaluation of Tree Structures on Flash SSDs，VLDB'21

- 最新论文分析：ATC’21，OSDI‘21，VLDB’21，SIGMOD‘21

#### 论文1-1（索引）：ArkDB: a Key-Value Engine for Scalable Cloud Storage Services

- sigmod 2021, 阿里盘古，**Bw-Tree + LSM-tree** 结合

- 背景：LSM-tree 和 Bw-tree

  - 传统的基于b+tree的存储引擎，例如BerkeleyDB和MySQL的innodb，读的性能很高，随机读和范围读都挺高，但是写性能比较低，因为写的时候，虽然只需要把redo log写入磁盘即可完成，但是为了缩短失效恢复时候的恢复时间，一般都需要频繁的做checkpoint。checkpoint是随机的写，随机写无论是在传统的机械硬盘还是SSD，或者PCID SSD上性能都较低，在实际测试MySQL的过程中使用TPCC，通常能把磁盘写满，导致刷redo log性能降低，从而影响系统整体的吞吐量
  - 而基于LSM-tree典型的例如leveldb／rocksdb，写入性能较高，但是读性能，特别是范围读能力性能较差，例如读一条数据首先要看memtable中有没有，然后再扫描1级sstable和2级sstable，则需要多次磁盘io才能读到这条数据，特别是1级sstable每个分块的range还可能交叉，需要扫描多个1级sstable文件。另外一个就是在compact的时候会有一个性能的剧烈波动。

- The Bw-Tree: A B-tree for New Hardware Platforms，ICDE’13，微软研究院

  - 通过无锁的方式来操作b+tree，提升随机读和范围读的性能。核心的思想是把b+tree的page通过page id（PID）映射map，map的[key, value]变成[PID, page value]，把直接对page的修改，变成一个修改的操作记录，加入到“page value”，所以“page value”可能是一个“base page”即page原始的内容，和一串对page修改形成的记录的链表，而在修改记录链表中加入一个修改记录节点可以很容易变成一个无锁的方式来实现。另外就是对btree的split和merge操作也通过类似的原理，把具体的操作细化成好几个原子操作，避免传统的加锁方式

    <img src="..\..\photos\paper\image-20211015104643876.png" alt="image-20211015104643876" style="zoom: 33%;" />                   <img src="..\..\photos\paper\image-20210820112030016.png" alt="image-20210820112030016" style="zoom: 50%;" />

  - 把传统checkpoint刷page的变成通过log struct storage方式刷盘，把随机写变成顺序写，提高写的性能

- ArkDB

  - 用户事务的数据更改记录在 UserTransactionLog 中，并在提交时应用于Active的 MemoryBuffer (❶)。 MemoryBuffer 支持point查找和有序范围扫描。 一旦Active MemoryBuffer 中的数据量达到阈值，例如 64 MB，这个 MemoryBuffer 就会被sealed，并打开一个新的 MemoryBuffer 以接受新的修改。 Sealed MemoryBuffer 被刷新到存储中 (❷)。 此刷新操作称为获取用户checkpoint。 存储的主干结构是一棵 B+ 树。 对于用户checkpoint，同一叶子page的数据更改形成了Host page的Delta Page。 Delta Page被写入 DeltaPageStream。 通过从Host Page到添加到 PageMappingTable 的这个 delta page的页面映射，一个 delta page被间接地挂接到 B+ 树中
  - 当Leaf Page的Delta Page数达到阈值时，该Leaf Page及其更改被压缩到单个连续Page中。稍后，该页面的存储位置将更新到父页面，导致父页面被重写。同样地，从旧的父页面映射到重写的父页面的页面被添加到PageMappingTable中。这个映射意味着父节点的实际内容现在位于重写的父节点页面中。更新了父页面后，可以删除压缩页的页面映射项。通过这种方式，ArkDB有效地收缩了PageMappingTable。当一个Host Page被压缩时，它可能被重写为多个页面，在这种情况下，它表示一个 Page Split。紧凑的叶页和重写的父页都被称为钩在B+树中的基页，并被写入BasePageStream中
  - 页面映射更改作为系统事务执行，并记录在SystemTransactionLog中。只有在系统事务日志记录被持久化之后，对PageMappingTable的更改才会被应用（❹）。从概念上讲，DeltaPageStream、BasePageStream和PageMappingTable一起形成了一个存储上的Bw-tree，在只追加的存储上有Delta Page。通过合并来自MemoryBuffers和存储上的Bw-tree的结果来回答查询
  - 随着用户检查点、页面压缩和页面拆分或合并，DeltaPageStream和BasePageStream中的extent也会被消耗掉。当一个页面被重写时，该页面先前写入的部分将失效。每个extent的确切有效用法在ExtentUsageTable中被跟踪，在DeltaPageStream和BasePageStream的垃圾收集期间会参考这个表。当ExtentUsageTable占用的内存达到一个阈值时，历史使用数据会被压缩，并溢出到存储(实际上是BasePageStream)，以控制表内存占用低于一个阈值
  - 当系统事务日志大小达到阈值时，PageMappingTable和ExtentUsageTable都会被刷新到系统检查点。在获取系统检查点之后 (❺) ，它的元数据(包括时间戳和位置)被记录到MetadataLog中。恢复从MetadataLog指示的最近的系统检查点开始。分区拆分或合并操作也记录在MetadataLog中，以指导恢复
  - 具体优化：略

  

#### 论文1-2（索引、测试）Toward a Beter Understanding and Evaluation of Tree Structures on Flash SSDs  

- Persistent tree data structures (PTSes) 
  - log structured merge (LSM) tree , used, e.g., by RocksDB and BigTable;
  - the B+Tree , used, e.g., by Db2 and WiredTiger (the default storage engine in MongoDB ); 
  - the Bw-tree,used, e.g., by Hekaton and Peloton; 
  - the B-e tree,used, e.g., by Tucana and TokuMX.

- 由于树结构的内部行为对性能的影响，以及 SSD 底层逻辑的特殊性，**SSD 上对数结构进行基准测试是一个复杂的过程，容易出现不准确的评估。**本文通过 RocksDB 和 WiredTiger 确定了 7 个 SSD 基准测试陷阱，该陷阱会导致对关键性能指标的错误测量，导致生产环境中无法实现最优部署。本文还提供了避坑指南来获得更可靠的性能测量结果
- 参考文档《[RocksDB测试方法]()》(doing)
  - Characterizing, Modeling, and Benchmarking RocksDB Key-Value Workloads at Facebook（Fast’20）：负载相关

#### 论文3-1（KV分离）Differentiated Key-Value Storage Management for Balanced I/O Performance

- ATC’21，李永坤团队，把KV分离玩出花了
- KV分离问题
  - GC低效：HashKV
  - Scan性能差：DiffKV，把value也用类树结构管理

<img src="..\..\photos\paper\image-20211014185307811.png" alt="image-20211014185307811" style="zoom: 67%;" />         <img src="..\..\photos\paper\image-20211014184813109.png" alt="image-20211014184813109" style="zoom:50%;" />

#### 论文3-2 (读优化-cache)： **Leaper: a learned prefetcher for cache invalidation in LSM-tree based storage engines**

- 北京大学，VLDB'20，采用机器学习方法解决X-Engine中cache miss问题

- 在X-Engine实际运行中，由于后台异步数据合并任务造成的大面积缓存失效问题 LSbM-Tree提出这种问题，具体解决是多增一个buffer cache，空间换效率。

- Leaper采用机器学习算法，预测一个 compaction 任务在执行过程中和刚刚结束执行时，数据库上层 SQL 负载可能会访问到数据记录并提前将其加载进 cache 中，从而实现降低 cache miss，提高 QPS 稳定性的目的

  <img src="..\..\photos\paper\image-20211014181914704.png" alt="image-20211014181914704" style="zoom:50%;" />            <img src="..\..\photos\paper\image-20211014181803802.png" alt="image-20211014181803802" style="zoom:50%;" />

- Leaper分成三个部分：Collector、Prefetcher和Learner

  - Collector：收集数据和分key range
  - Prefetcher：预测Hot key
  - Learner：学习模型/训练

#### 论文3-2 (读优化-filter)： Rosetta: A Robust Space-Time Optimized Range Filter for Key-Value Stores && **Chucky: A Succinct Cuckoo Filter for LSM-Tree**

- 哈佛大学[Stratos Idreos](https://stratos.seas.harvard.edu/)团队，Monkey，Dosteovsky，，，
- bloom filter的问题
  - Chucky（sigmod'21）：性能低，开销大。之前的SSD或者HDD等访问延迟较大，Bloom filter引起的开销比重较小，可以忽略不计NVMe SSD性能提升，与DRAM性能差距进一步缩小，由于在每层都要维护Bloom filter，会引起比较大的查询延迟，引起LSM-Tree中Bloom filter成为新的瓶颈之一  
  - SuRF，Rosetta（sigmod'20）：不支持范围查询
- Rosetta：优化SuRF的范围查询能力
  - 问题1：Short and Medium Range Queries are Significantly Sub-Optimal
  - 问题2：Lack of Support for Workloads With Key Query Correlation or Skew

<img src="..\..\photos\paper\image-20211014182934755.png" alt="image-20211014182934755" style="zoom:50%;" />

- Chucky
  - 关注点：高性能介质的使用，Bloom filter引起的开销不容忽视
  - Chucky提出用单个Succinct Cuckoo Filter替代LSM-Tree中的多个Bloom filter，可以有效减少查找引起的开销

### 主题2：OSD架构优化

- 参考文档《[Ceph优化总结](../Object store/2020-07-25-Ceph优化总结.md)》

- Messenger优化
  - 问题：当连接过载时，它将导致工作线程之间出现负载不平衡的问题
  - Optimizing communication performance in scale-out storage system，平衡算法&双工作线程
  - Async-LCAM：缓解锁争用开销
- Ceph-osd流程优化
  - Performance Optimization for All Flash Scale-Out Storage：主要针对写过程，细化PG锁流程，无阻塞日志和轻量级事务处理
  - Design of Global Data Deduplication for a Scale-Out Distributed Storage System：重复数据删除
  - Re-architecting Distributed Block Storage System for Improving Random Write Performance，ICDCS‘21
  - Crimson: A New Ceph OSD for the Age of Persistent Memory and Fast NVMe Storage：社区方向 Seastore
  - Ceph Optimizations for NVMe：RocksDB去重 + spdk、dpdk与ceph messenger RDMA
- 后端存储 Bluestore & Bluefs
  - File systems unfit as distributed storage backends: lessons from 10 years of Ceph evolution
  - Reconciling LSM-Trees with Modern Hard Drives using BlueFS
- 其他
  - Towards Cluster-wide Deduplication Based on Ceph
  - SAPPHIRE

#### 论文1：Re-architecting Distributed Block Storage System for Improving Random Write Performance

- ICDCS‘21，Myoungwon Oh 团队，三星&国立首尔大学，通过重新架构Ceph来构建一个具有高随机写性能的分布式块存储系统
- 主要思想和设计原则参考KVell：使用分片分区（share-nothing），随机IO不排序

- 基于NVMe SSD的Ceph性能瓶颈

  - roofline-based 的方法对带有NVMe固态硬盘的传统分布式数据块存储系统（Ceph）进行性能分析，发现**瓶颈不在于某个特定的软件模块，而在于整个软件堆栈**
    - Roofline: an insightful visual performance model for multicore architectures.  
  - (1) tightly coupled I/O processing
  - (2) inefficient threading architecture
  - and (3) local backend data store causing excessive CPU usage  

- 背景以及问题

  - OSD守护进程由三个模块组成: Messenger、OSD core，后端存储

    - Messenger：有messenger 线程池，负责消息处理(MP)的，收到IO请求，它会将请求插入到相应的PG工作队列中
    - OSD core：有PG线程池，负责PG工作队列中传入请求的复制处理(RP)、事务处理(TP)
    - Bluestore：后端对象存储(OS)，负责数据持久化。使用RocksDB存储Ceph的元数据，同时将用户数据存储在原始磁盘中。由于数据是以 out-of-place 的方式更新的，因此需要维护工作(MT)，例如compaction
    - 大致分成五个过程消息处理(MP)、复制处理(RP)、事务处理(TP)、对象存储(OS)和维护任务(MT)

  - 性能测试：baseline Ceph (Original) 和三个修改版本

    - RTC-v1：实现了一个run-to-completion (RTC) 模型，一个线程执行客户端请求的完整处理，它包括MP，RP，TP，OS，MT
    - RTC-v2：不存在对象存储开销时RTC模型的性能，对后端对象存储的写请求立即返回成功，只包括MP、RP和TP
    - RTC-v3：当事务处理和对象存储相关工作不产生开销时的性能，只包括MP和RP

    <img src="..\..\photos\paper\image-20211014152126870.png" alt="image-20211014152126870" style="zoom:50%;" />

    - NP：用于Network Processing 的CPU时间，包括MP和RP的；latency-critical jobs
    - SP：代表 Storage Processing 的CPU时间， 包括TP和OS。
    - 当使用RTC模型时，RTC代表网络处理和存储处理的CPU时间部分，即RTC = NP + SP
    - MT：表示后端对象存储所需的维护任务的CPU时间，例如 compaction 和 sync 等等
    - 总结一下
      - RTC-v1执行MP+RP+TP+OS+MT，RTC-v2执行MP+RP+TP，RTC-v3执行MP+RP
      - NP = MP + RP；SP = TP + OS；RTC = NP + SP

  - 性能分析

    - fio，4KB随机写入
    - Original配置：**两个messenger线程**和**两个PG线程**，同时将Ceph process 设置为仅在4个内核上运行；为了公平比较，将Ceph使用的线程总数保持为**每个节点四个线程**，不包括后端对象存储使用的线程数
    - 在原始版本中，每个节点的CPU使用率为346%，而性能为29K IOPS
    - 假设其性能随系统内核数量线性扩展，最理想的性能约为369K IOPS(每个节点有44个逻辑内核，29*11)，非常低
    - 每个节点使用8个NVMe SSD（4K随机写 330KIOPS）

  - 问题分析 和 解决方法

    <img src="..\..\photos\paper\image-20211014160938315.png" alt="image-20211014160938315" style="zoom: 67%;" />

    - **Tightly Coupled I/O Processing** 
      - 1.主节点从客户端接收请求；2. 主节点将数据保存在存储器中；3. 主节点向副本服务器发送复制请求；4. 辅助节点在收到复制请求后将数据保存在存储中；5. 主节点等待来自副本的输入/输出完成确认；6. 主节点将结果发送给客户端
      - RTC-v3  其平均延迟(0.8毫秒)也高于的NVMe SSD的4KB随机写入延迟(0.4毫秒)，且CPU使用率为200%。这意味着除了网络处理或延迟复制处理之外，提交写入的额外写入过程会导致高延迟，并且IO路径已经使用了大量的CPU
      - 问题在于 latency-critical tasks 和 the best-effort batch tasks 紧耦合
      - **优化措施：**2、4过程 和 其他过程 需要解耦，异步flush

    <img src="..\..\photos\paper\image-20211014160151521.png" alt="image-20211014160151521" style="zoom:50%;" />

    - **Inefficient Threading Architecture**

      - 大多数分布式存储系统使用线程池模型，它为不同的目的管理不同的线程池

      - 在Ceph中，有两个线程池: 用于网络处理的messenger线程池和用于存储处理的PG线程池

      - 然而，这种体系结构导致**线程之间频繁的上下文切换**，因为IO请求是在一条长路径上处理的，其中messenger和PG线程都多次涉及，从而导致上下文切换开销，这在快速设备中变得不可忽略

      - 为了解决快速设备的性能问题，RTC model被应用，通过最小化上下文切换开销和提高缓存效率

        - IX: A protected dataplane operating system for high throughput and low latency, 应用到网络解决方案中，其中每个数据包在处理任何其他数据包之前都被完全处理
        - Seastore

      - 简单的RTC模型 RTC-v1

        - 用一个RTC模型替换Ceph中的传统线程池模型，其中每个RTC线程都被固定在一个专用的内核上。请注意，为了防止来自单个客户端的请求重新排序，来自同一客户端的所有请求都由同一messenger线程处理。因此，在正在进行的IO操作完成之前，RTC 线程无法处理其他IO请求，它会导致响应时间延迟
        - 与Original相比，简单的RTC模型(RTC-v1)通过减轻上下文切换开销，在较低的CPU使用率下实现了稍好的性能。此外，没有对象存储和维护任务的简单RTC模型(RTC-v2)实现了较高性能
        - 但是，它的延迟仍然比NVMe慢(1.45毫秒)，并且性能提升很小，因为RTC线程在等待副本的写完成确认时被阻塞

      - **优化措施：**Prioritized Thread Control

        - 为不同的线程类型维护单独的CPU池，涉及线程调度策略，以提高CPU利用率

        - 有两种线程类型：latency-critical tasks的优先级线程 和 其他任务的非优先级线程，前者负责消息交换和复制处理，而后者负责存储处理

          <img src="..\..\photos\paper\image-20211014162824062.png" alt="image-20211014162824062" style="zoom:50%;" />

        - 每个优先级线程都被固定在一个专用的核心上，因为来自其他线程的干扰会影响客户端感知的性能。相比之下，非优先级线程共享剩余的内核，每个内核负责PG的存储处理

        - 这种线程架构背后的一个关键概念是为延迟关键任务赋予高优先级，以便写操作可以以低延迟(通过优先级线程)完成，同时通过以批处理方式(通过非优先级线程)刷新写操作来减少CPU消耗

        - 具体线程设计和调度 略

    - **Local Backend Object Store causing High CPU Usage**

      - Ceph的默认后端对象存储 BlueStore  直接管理存储设备，而不使用本地文件系统来解决问题，例如日志异常的日志记录，以及分布式存储系统中的对象和文件系统中的文件之间不必要的转换开销。但是，在使用NVMe固态硬盘时，在随机IO工作负载下，**BlueStore 可能会受到CPU的限制**
      - 这主要有两个原因。第一，主机端写放大。BlueStore使用基于LSM-tree的**RocksDB**进行元数据和小数据写入。然而，所有LSM-tree需要compaction；此外，虽然它是在后台执行的，但它不仅干扰前台I/o，而且需要很高的CPU时间来运行它。后端数据存储的后台作业(MT)占总CPU使用量中不可忽略的一部分
      - 第二，单一data domain。现代后端数据存储使用的**一致性机制**可能是可伸缩性的性能瓶颈。例如，BlueStore需要同步原语来处理单个分区内的事务。为了克服这些问题，一些研究提出了一种用于本地文件系统的分区域方法，然而，由于本地文件系统不向应用程序公开这种分区域信息，分布式存储系统很难实现位置感知处理
      - **优化措施：**CPU-efficient object store
        - 旨在最大限度地减少执行客户端发出的操作所需的IO数量，并在存储处理过程中最大限度地提高并行性
        - **避免写入放大**，它为每个分区使用就地更新磁盘布局，例如日志文件系统；它由元数据、块图和数据区组成。因为它允许覆盖，所以不需要清理过程（compaction），它不仅降低了主机端的写入放大，还降低了主机端的CPU消耗
        - 使用了**预分配技术**，用于避免不必要的元数据更新，减少IO数量
          - WALDIO: Eliminating the filesystem journaling in resolving the journaling of journal anomaly  
          - 覆盖预先分配的对象不需要对其元数据(如块位图、索引节点表等)进行任何更改，不需要元数据更新，这只有在对象大小固定的情况下才可行。在典型的数据块存储服务中，数据块设备映像被分条到固定大小的对象上。例如，在Ceph RBD，默认对象大小为4MB。因此，可以在创建时预先分配所有对象
        - 为了获得高性能，对象存储使用**NVM作为元数据缓存**。它还将整个磁盘空间划分为多个分区，这样就可以将一个非优先级线程分配给一个分区。因此，可以并行处理IO操作，而不会发生锁争用

### ~~主题3：新介质和新场景的~~

### 其他

- 自动调参：参数对系统使用影响非常大，Ceph相关研究很少
- 可靠性：数据分布，故障处理
- SCM、RDMA

#### 论文（NVM）：Improving Performance of Flash Based Key-Value Stores Using Storage Class Memory as a Volatile Memory Extension  

- Optane PMem 100系列SCMs (DCPMM) 做 RocksDB 混合缓存

<img src="..\..\photos\paper\image-20211014202654577.png" alt="image-20211014202654577" style="zoom:50%;" /><img src="..\..\photos\paper\image-20211014202953402.png" alt="image-20211014202953402" style="zoom: 33%;" />

- 应用场景（doing）



## 思考

- 之前的想法
  - 高性能KV，针对场景和设备
  - ~~混合存储中，性能的提升、数据交换~~
  - ~~混合存储中，从延迟上保证前端io的情况下，尽可能给后台任务分配更多的资源~~

> 目标要小，想法要大 — 冉·阿万

- **以RocksDB为基准，LSM-tree的读性能优化为目标**

  - 思考1：LSM-tree的读性能是否是个问题？

    - 传统的读性能问题指的是要一层层找，导致多次IO和读放大
    - RocksDB很多优化，参数调控和Cache设置，可以保证一次找到
    - RocksDB关注的优化点（Fast‘21）：
      - How can we use **SSD/HDD hybrid storage** to improve efﬁciency? （混合存储）
      - How can we mitigate the performance impact on readers when there are many consecutive **deletion markers**? （标记删除对读性能的影响）
      - How should we improve our **write throttling algorithms**? （节流算法，写操作和compaction导致的性能停顿）
      - Can we develop an efﬁcient way of comparing two replicas to ensure they contain the same data? （一致性算法）
      - How can we best exploit **SCM**? Should we still use LSM tree and how to organize storage hierarchy? （新介质）
      - Can there be a generic integrity API to handle data handoff between RocksDB and the ﬁle system layer? （RocksDB和文件系统之间的数据转换，提供通用API）

  - 思考2：索引、cache、filter等等，应该从何下手？

    - 分析RocksDB火焰图，seek和读盘 -> 思考索引的优化

    - 有些细节可以扣，比如序列化、系统调用开销等等 -> 内存组织 / spdk / by pass kernel

      <img src="..\..\photos\paper\rocksdb.svg" alt="rocksdb" style="zoom:50%;" />

    - cache的影响？和负载有关，随机读cache? -> Scan操作的cache优化？

    - RocksDB Filter的误报率比较低，可能的问题是内存消耗和Scan -> Learned index也有做filter的研究

  - 思考3：继续之前的研究，更加深入？

    - learned index训练和模型的优化

    - google的方式可以使得性能与模型精度无关，但是sstable创建时训练模型，占用过多CPU，严重影响写性能和compaction

    - delete问题：分区的方法治标不治本，同一分区内的删除也会有问题；另外分区后需要额外维护索引来加速Get和Scan，导致写性能下降

      <img src="..\..\photos\paper\image-20211013171124220.png" alt="image-20211013171124220" style="zoom: 33%;" />               <img src="..\..\photos\paper\test14_gain.png" alt="test14_gain" style="zoom:42%;" />   

  - 思考4：另辟蹊径？

    - 从CPU占用入手？Scan性能？NVM能干嘛？
    - bluefs？索引结构能不能结合一下？
    - 新问题？新场景？

## 参考文献

1. 无锁B-tree：http://mysql.taobao.org/monthly/2018/11/01/

2. Bw-tree：https://15721.courses.cs.cmu.edu/spring2017/papers/08-oltpindexes2/bwtree-icde2013.pdf

3. Roofline: an insightful visual performance model for multicore architectures.  

   

10月21日-23日，在珠海的高性能计算年会；
10月28-30日，在深圳的中国计算机大会；
11月5-7日，在杭州的计算机系统大会，
11月18-20日，在广州的大数据年会；

老师同学们大家好，我叫鲁凯，是18级直博生，我申请的是一等知行奖学金。申请理由有三个：一是，成绩优异，加权86.97，排名是第三（14个博士），上学年顺利完成博士开题和中期答辩；2，科研成果方面，上学年以第一作者完成两项论文研究，一项已经被CCF A类期刊TPDS录用，另一项已经投稿CCF B类会议IPDPS，目前还在审稿中；3，项目工作方面，上学年带队负责和商汤科技研究院合作的，高性能键值存储系统的一个项目，输出部分核心代码和多篇论文以及专利。主要就是这些，谢谢大家。



应该是我之前漏写了，很早之前写的申请表，后来出差就没搞了

