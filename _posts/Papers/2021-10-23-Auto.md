# ACSTS: Automatic Cloud Storage Tuning System using Reinforcement Learning 

> 我必须考虑，这会不会是我此生仅有的机会 — 达纳德·许



- 主题

  - 强化学习：ACSTS: Automatic Cloud Storage Tuning System using Reinforcement Learning 

  - 负载变化+迁移学习：ACSAT: Adaptive Cloud Storage Auto-Tuning System using Transfer Learning

  - AutoML中经验：Auto-of-Auto: Automatic Cloud Storage Tuning with AutoML

    

## 冲！

- 目标会议：DAC 2021，11/15，11/22
- 三部分 （10/29，还剩**15~22**天）
  - Abstract + Introduce + Related Work：2页 （98%）
    - 差abstract
  - **Design + Implementation**：2页 （30%）
    - 参数选择完成
    - 写完代码，收敛困难
      - 模型优化：分布式A3C？
      - 学习：https://github.com/hibayesian/awesome-automl-papers
    - 训练优化：？~~分段式 + 参数复用~~
      - 分布式训练 + 状态机
  - Evaluation + Conclusion + References：2页 （0%）
- **NEXT**：训练优化 + 创新思考



## Abstract



## Introduce

- （云存储）

**【背景】**

With the explosive growth of data, cloud systems are becoming more and more complex and can be adopted to suit various types of use cases and users make use of cloud resources in very distinct ways.  

Cloud storage services provide cost-effective, highly scalable and reliable platforms for storing large scale enterprise data due to the underlying object-based storage technology (e.g OpenStack Swift [4], Ceph [26], Amazon S3, etc.).  

由于复杂的云环境，包括负载和设备的变化，很困难这里的技术解决方案的主要动机是试图回答云操作人员如何更改存储配置以更好地支持这些特定的访问模式的问题。Often, storage systems are deployed with default configurations, rendering them sub-optimal. Finding optimal configurations is difficult due to the numerous combinations of parameters and parameter sensitivity to workloads and deployed environments  

默认的配置参数往往不能保证集群资源的充分利用和系统的高吞吐率，因此需要调整参数配置从而提高系统在吞吐量、能耗、运行时间等方面的性能。



**【困难】**

相比传统的数据库或者，determining the optimal number of microstores and performing workload-to-microstore mapping on the fly is challenging in a multi-tenant cloud environment.   

庞大的参数空间：参数众多，不好的参数带来性能。。

- 参数多，传统的黑盒
- 干扰因素很多
- 变化莫测的负载和请求

Emerging cloud systems are facing great challenges in performance, scalability and reliability\cite{ceph,random}. Cloud systems are becoming more and more complex and can be adopted to suit various types of use cases and users make use of cloud resources in very distinct ways.  

这个html怎么说呢，一些参数看不懂查一下就行，比如这个filter，就是过滤分类的

In recent years, large-scale data centers host a large number of applications that serve several million users. To meet this requirement, the scale-out storage architecture, distributed object storage, has been widely used in modern storage systems and becomes the underlying storage layer. In the supercomputing center, the well-known cluster file systems (e.g., GPFS [1] and Lustre [2]) use object storage to store both data and metadata. In the cloud platform, Amazon Simple Storage Service (S3) and Microsoft Azure [3] use object storage to support various applications. The hybrid storage system, Ceph [4], also uses object storage to construct various storage systems. Therefore, improving the performance of object storage is extremely salient for improving data center access efficiency



Cloud storage services provide cost-effective, highly scalable and reliable platforms for storing large scale enterprise data. This is possible due to the underlying object-based storage technology (e.g OpenStack Swift [4], Ceph [26], Amazon S3, etc.). However, todays cloud services are associated with various performance issues [21, 27]. One of the factors influencing cloud storage performance is the interference from background tasks such as data scrubbing, recovery, rebalancing etc. running on a storage cluster. Furthermore, the difference in the processing capabilities of storage servers, which arises as servers are gradually upgraded and replaced in a cloud datacenter, can also be detrimental to the overall performance of the storage cluster. Existing cloud storage systems mostly rely on human operators to tune various control knobs to address performance issues. For example, a datacenter administrator needs to tune various configuration parameters to determine load balancing and data distribution strategies for the storage cluster. Manually tuning these control knobs for maintaining optimal performance is not only burdensome but may also be ineffective if they are not adapted to diverse workload patterns. In our motivational case study on an opensource object storage system, Ceph, we observed a complex interplay between various workload patterns (i.e. read-write ratio, data object size, etc.), underlying resource bottlenecks, and the performance of storage cluster under different system
adaptation strategies. We also observed that the positive effects of an applied strategy may not be immediately visible, which
further complicates the task of performance tuning  



**【我们的办法】**

Storage systems come with a large number of configurable parameters that control their behavior. Tuning such parameters can provide significant gains in performance, but is challenging due to huge spaces and complex, non-linear system behavior. Auto-tuning with black-box optimization have shown promising results in recent years, thanks to its obliviousness to systems’ internals.
However, previous work all applied only one or few optimization methods, and did not systematically evaluate them. Therefore, in this thesis, we first apply and then perform comparative analysis of multiple black-box optimization techniques on storage systems from various aspects such as their ability to find near-optimal configurations, convergence time, and instantaneous system throughput during auto-tuning, etc. We also provide insights into the efficacy of these automated black-box optimization methods from a system’s perspective. 



Most storage systems come with large set of parameters to directly or indirectly control a specific set of metrics that may include performance, energy, etc. Often, storage systems are deployed with default configurations, rendering them sub-optimal. Finding optimal configurations is difficult due to the numerous combinations of parameters and parameter sensitivity to workloads and deployed environments. Construction of such a complex system comes with many design choices, producing a large number of configurable parameters [30]. Figure 1 depicts the number of Ceph parameters grows drastically to over 1500, near three times larger than the original version. But storage systems are often deployed with default settings provided by developers, rendering to be suboptimal for specific user cases   





## Related Work



**【存储系统自动调参】**

**SAPPHIRE、Cao、impact**

In this section, we describe the related auto-tuning studies. In recent years, several studies were made to automate
the tuning of all kinds of computer systems [？]. Jian et al. [？] use neural networks to optimize the memory allocation of database instances, by adjusting buffer pool sizes dynamically according to the miss ratio. Ashraf et al. [？] perform a cost-benefit analysis to achieve long-horizon optimized performance for clustered NoSQL DBMS in the face of dynamic workload changes. Ana et al. [？] try to recommend near-optimal cloud VM and storage hardware configurations for target applications based on
sparse training data. Black-box optimization are used, as they view the system in terms of its inputs and outputs and assume obliviousness to the system internals. Methods like Simulated Annealing [9], Genetic Algorithms [2], Reinforcement Learning [37], and Bayesian Optimization [17, 26] are implemented to find near-optimal configurations. Zhen et al. [5, 7, 8] tries to auto-tune storage systems to improve I/O performance. They summarize the challenges in tuning storage parameters and then perform
analysis of multiple black-box optimization techniques on the storage systems. Their works mainly focus on the local storage systems, often with less than 10 parameters. While in our work, we focus on the distributed storage systems and find new challenges, such as the configuration constraints, the huge numbers of parameters, and the higher noise.



Caver [？] also tries to solve the challenge of the large number of parameters and exponential number of possible configurations. Like SAPPHIRE, Caver proposes to focusing on a smaller number of more important parameters. Inspired by CART [4], Carver uses a variance-based metric to quantify storage parameter importance. Carver is designed for categorical parameters, as they find most parameters in local storage systems are discrete or categorical. But we observe the exact opposite in Ceph, as most configurable parameters are continuous (about 90 percent). From Table 2 we can find that all the top 16 parameters are continuous. Although there are discretization techniques that can break continuous parameters into discrete sections, feature-selection results depend heavily on the quality of discretization [？]. Thus, Carver is not suitable for our problem. Different from Carver, SAPPHIRE leverages Lasso to choose important knobs. Lasso can provide higher quality results for continuous parameters. And a small number of categorical parameters would not degrade the parameter ranking’s performance.



**【结合强化学习】**

**ottertune、cdbtune**

SmartConf [？] try to auto-adjust performancesensitive parameters in the distributed in-memory computing system, spark, to provide better performance. SmartConf uses a control-theoretic framework to automatically set and dynamically adjust parameters to meet required operating constraints while optimizing other system performance metrics. But SmartConf does not work if the relationship between performance and parameter is not monotonic. While in our cases, based on Figure 2b, those relationships can be irregular and multi-peak. Unlike SmartConf, SAPPHIRE uses machine learning techniques, which is a better fit for such complicated configuration space to find near-optimal settings. DAC [33] finds that the number of performancesensitive parameters in spark is much larger than previous related studies (more than 40 vs. around 10). DAC combines multiple individual regression trees in a hierarchical manner to address the high dimensionality problem. To reduce the modeling error, the hierarchical modeling process requires a large number of training examples, which is proportional to the number of parameters. But there are hundreds of performance-related parameters in our problem comparing to 40 in DAC. Modeling such a highdimensional system with DAC would require hundreds of hours to collect training examples, which is impractical  



## Design 





**【系统介绍】**

unconfigurable

configurable： Static and dynamic configuration

<img src="..\..\photos\RL\RLRP.png" alt="RLRP" style="zoom:18%;" />

- 优先级队列：参数分类和筛选关键参数
- 强化学习：关键参数调整
- 训练优化：？

**【参数筛选&&选择】**

<img src="..\..\photos\paper\image-20211027200230711.png" alt="image-20211027200230711" style="zoom:50%;" />

**【强化学习模块】**





**【训练加速】**

<img src="..\..\photos\RL\train_RL2.png" alt="train_RL2" style="zoom:18%;" />

##  Implementation  



ACSTune是在Park \cite{Park}平台上实现的，这是一个面向学习增强的计算机系统的开放平台。 RLRP的所有代码都是用c++和python编写的，并基于张量流实现了强化学习模型 。此外，ACSTune被成功打包到实际的分布式存储系统Ceph (v12.2.13) \cite{Ceph}中。 如图. \ref{Ceph}所示，Mertics收集器和动作控制器通过Ceph Monitor与Ceph交互。 Mertics Collector通过使用Linux SAR (system Activity Report) \cite{SAR}实用程序，每隔30秒从Ceph osd获取系统Mertics。 Action Controller调用Ceph监视器来实现RL Agent所做的放置/迁移操作，并更新Ceph集群的OSDmap。 RLRP以插件的形式实现，并保留了Ceph原有的体系结构和其他流程。  

<img src="..\..\photos\RL\RLRP-Ceph.png" alt="RLRP-Ceph" style="zoom:18%;" />

ACSTune is implemented on Park \cite{park} platform, which is an open platform for learning-augmented computer systems. All the codes in RLRP are written in C++ and python, and the Reinforcement Learning model is implemented based on the Tensorflow. In addition, ACSTune is successfully packaged into the actual distributed storage systems, Ceph (v12.2.13) \cite{ceph}. As shown in Fig. \ref{Ceph}, the Mertics Collector and Action Controller interact with Ceph through the Ceph Monitor. Mertics Collector fetches the system mertics from the Ceph OSDs at 30 second intervals by using the Linux SAR (System Activity Report) \cite{sar} utility. The Action Controller invokes the Ceph monitor to implement the placement/migration actions made by the RL Agent and update the OSDmap of Ceph cluster. RLRP is implemented as plug-ins, and retains the original architecture and other processes of Ceph.



## Evaluation





- 测试平台：Cloudsim、COSBench、fio/rados benchmark

  https://github.com/intel-cloud/cosbench

- Ceph

- 真实数据



配股，我看这个文件夹里面没有crosshairs的图片，但是它的效果就是有，我直接把图片改成industry直接就没显示了



## 讨论



