# 大模型相关调研

> 说那六贼，一个唤做**眼看喜**，一个唤做**耳听怒**，一个唤做**鼻嗅爱**，一个唤作**舌尝思**，一个唤作**意见欲**，一个唤作**身本忧** —— 老吴



## 问题

- 当前的问题：
  - lustre上训练有时起不来，有时会卡住，ceph训练比cache慢
  - 大模型参数卸载到主存或者磁盘上
  - checkpoint 怎样快速的保存

- 整体规划
  - **NLP榜单登顶**，主要涉及到算法，如何支持超过2000亿参数的模型训练
  - **在顶会发表paper**
  - 重构整个训练及推理框架，6个月后开源

- 下一步计划：

  - 分析使用lustre和ceph相关的问题

  - 共享资料，分析啊现有的优化方法，分析现在大模型的训练瓶颈，寻找优化点

  - 根据分析结果确定优化路径，时间点

    

## 第一代sensetime-LM训练框架

### 测试

#### 1.环境搭建（集群1424，分区Model）

- 不能连外网 + 换了新仓库
- 运行环境（cuda-11.3，.bashrc，.local）
  - 拷贝conda环境：/mnt/cache/share/spring/conda_envs/miniconda3
  - 修改.local：直接复制/mnt/cache/share_data/zhanmingjie/.local_zhanmingjie_2021121501
  - 修改.bashrc
    - 直接复制/mnt/cache/share_data/zhanmingjie/.bashrc
    - 修改cuda路径：/mnt/cache/share_data/zhanmingjie/cuda-11.3
    - 修改.local路径：.local
    - source ~/.bashrc
- pypi依赖安装
  - regex deepspeed nltk numpy pandas sentencepiece boto3 torch tqdm jieba pybind11
  - pypi新仓库 https://pkg.sensetime.com/repository/pypi-proxy/simple/ 
  - pip install -i https://pkg.sensetime.com/repository/pypi-proxy/simple/ --trusted-host pypi.opencloud.sensetime.com regex
- conda依赖安装：nvidia-apex
  - export TORCH_CUDA_ARCH_LIST="compute capability"
  - conda新仓库：https://opencloud.sensetime.com/#/registry/conda
  - conda install -c conda-forge nvidia-apex
- 数据集：/mnt/cache/share_data/zhanmingjie/cpm-2-t5-data_8/...
- 运行脚本： src/scripts
- 当前问题：Model分区任务过多，没有资源

#### 2. 集群lg，分区pat_dev

- 环境：conda，cuda-11.3，apex，.bashrc，.local

- 当前问题：数据集过大，磁盘限额；放在ceph或者lustre，影响性能

  

#### 3. 预训练运行流程

- srun -p Model -N 8 --job-name=xm_10000 --gres=gpu:8 --cpus-per-task=64 --quotatype=spot bash pretrain_enc_dec_11b_fp32.sh
  -  slurm使用 https://confluence.sensetime.com/pages/viewpage.action?pageId=178346269
  - --partition=Test：提交任务到 Test 分区，也可以简写为 -p Test
  - --mpi=pmi2：提交 MPI 任务时的 MPI 类型，IntelMPI、Mvapich2、OpenMPI都可以使用 pmi2
  - --gres=gpu:4：单个节点使用的 GPU 个数，如果不需要 GPU，则忽略该项
  - --ntasks-per-node=4：单个节点最多的进程个数。在提交 SenseNet 时，每个进程占一个 GPU，所有应该和上面的 -gres 参数值保持一致
  - -n12：作业总的进程数，Slurm 会根据总进程数和 –ntasks-per-node 来决定具体分配多少个节点，然后根据 -gres 决定每个节点分配多少个 GPU
  - --job-name=HELLO：可选项，指定作业名称
  - --kill-on-bad-exit=1： 可选项，若作业中某个进程意外终止，则终止整个作业。默认情况下单个进程终止不会停止整个作业
- python -m torch.distributed.launch --nnodes 8 --node_rank 2 --nproc_per_node 8 --master_addr SH-IDC1-10-142-5-32 --master_port 6009 ../../../CPM-2-Pretrain/src/pretrain_enc_dec.py --use-new-lr --min-lr 0.0000006 --model-config ../../../CPM-2-Pretrain/src/configs/model/enc_dec_11b_config.json --model-parallel-size 1 --batch-size 48 --enc-seq-length 512 --dec-seq-length 256 --train-iters 300000 --save /mnt/lustre/lukai1/tmp/checkpoint/cpm-2-11b_n8_fp32_zero3_b48_lr2e6_20220112 --log-file /mnt/lustre/lukai1/tmp/log_11b_n8_fp32_zero3_b48_lr2e6_20220112.txt --load /mnt/lustre/lukai1/tmp/checkpoint/cpm-2-11b_n8_fp32_zero3_b48_lr2e6_20220112 --data-path /mnt/cache/share_data/zhanmingjie/cpm-2-t5-data_4/mix_cpm2_t5_document --data-impl mmap --split 950,30,20 --distributed-backend nccl --lr 0.000002 --no-load-optim --lr-decay-style linear --weight-decay 1e-2 --clip-grad 1.0 --warmup 0.01 --tokenizer-path ../../../CPM-2-Pretrain/src/bpe_cn --save-interval 1000 --eval-interval 500 --eval-iters 25 --log-interval 100 --checkpoint-activations --deepspeed-activation-checkpointing --deepspeed --deepspeed_config ../../../CPM-2-Pretrain/src/configs/deepspeed/ds_cpm2_11b_fp32.json

- 参数脚本：cmd_pretrain_enc_dec_11b.sh

### sensetime-LM代码

- CPM-2-Finetune：他们的代码
- CPM-2-Pretrain：他们的代码，deepspeed还未使用
  - data模块
- DeepSpeedExamples
  - Megatron-LM-v1.1.5-ZeRO3
    - megatron-new
      - data
        - T5-dataset
          - 调用了 helpers.cpp自己的mmap

### CPM-2-Pretrain

- pretrain_enc_dec：主流程

  ```python
  def main():
  
      # Disable CuDNN.
      torch.backends.cudnn.enabled = False
  
      # Timer.
      timers = Timers()
  
      # Arguments.
      args = get_args()
  
      os.makedirs(args.save, exist_ok=True)
  
      # Pytorch distributed.
      initialize_distributed(args)
      if torch.distributed.get_rank() == 0:
          print('Pretrain Enc-Dec model')
          print_args(args)
          with open(os.path.join(args.save, "args.json"), "w") as f:
              json.dump(vars(args), f)
  
      # Random seeds for reproducability.
      set_random_seed(args.seed)
  
      # setup tokenizer
      tokenizer = EncDecTokenizer(os.path.join(args.tokenizer_path, 'vocab.txt'))
      
      with open(args.deepspeed_config, "r") as f:
          ds_config = json.load(f)
  
      args.gradient_accumulation_steps = ds_config["gradient_accumulation_steps"]
      args.zero_stage = ds_config['zero_optimization']['stage']
  
      # Model, optimizer, and learning rate.
      model, optimizer, lr_scheduler = setup_model_and_optimizer(args, tokenizer.vocab_size)
      optimizer.cur_scale = 4096
      
      if torch.distributed.get_rank() == 0:
          print(args.iteration)
      
      # 数据集读取和划分
      train_data_iterator, val_data_iterator, test_data_iterator = \
              build_train_valid_test_data_iterators(
                      train_valid_test_dataset_provider, args, tokenizer)
  
      #训练
      iteration = 0
      if args.train_iters > 0:
          iteration, skipped = train(tokenizer, model, optimizer,
                                     lr_scheduler,
                                     train_data_iterator,
                                     val_data_iterator,
                                     timers, args)
  
          prefix = 'the end of training for val data'
          evaluate_and_print_results(tokenizer, prefix, val_data_iterator,
                                                    model, args, timers, False)
  	
      #模型保存 checkpoint
      if args.save and iteration != 0:
          save_checkpoint(iteration, model, optimizer, lr_scheduler, args)
  
      if args.do_test:
          # Run on test data.
          prefix = 'the end of training for test data'
          evaluate_and_print_results(tokenizer, prefix, test_data_iterator,
                                     model, args, timers, True)
  ```

  

- from data.enc_dec_dataset import build_train_valid_test_datasets

  - train_valid_test_dataset_provider调用

  ```python
  train_ds, valid_ds, test_ds = build_train_valid_test_datasets(
      tokenizer=tokenizer,
      data_prefix=args.data_path,
      data_impl=args.data_impl,
      splits_string=args.split,
      train_valid_test_num_samples=train_val_test_num_samples,
      enc_seq_length=args.enc_seq_length,
      dec_seq_length=args.dec_seq_length,
      seed=args.seed,
      skip_warmup=(not args.mmap_warmup))
  ```

  - data_impl: ['lazy', 'cached', 'mmap', 'infer']
  - warmup：训练开始的时候先选择使用一个较小的学习率，训练了一些epoches或者steps(比如4个epoches,10000steps)，再修改为预先设置的学习来进行训练

- 数据集

  ![image-20220113154054964](..\..\photos\paper\dataset1.png)

### data模块

- **读取数据集，生成训练、验证、测试数据**

  - enc_dec_dataset.py：数据集读取和格式处理

  - indexed_dataset.py：数据格式处理，包括np mmap 形式读取

  -  helpers.cpp：自己mmap实现

  - dataset_utils.py：compile_helper

  - **enc_dec_dataset**： build_train_valid_test_datasets

    - get_indexed_dataset_：读取三种数据context，target，target_offset

      - make_indexed_dataset

        - **indexed_dataset** ：make_dataset

          - infer：infer_dataset_impl，读取并根据magic，选择cached或者mmap

          - lazy：IndexedDataset

            - read_index()，read_data()

              ```python
              def read_index(self, path):
              	with open(index_file_path(path), 'rb') as f:
              		magic = f.read(8)
                      assert magic == self._HDR_MAGIC, (
                      'Index file doesn\'t match expected format. '
                      'Make sure that --dataset-impl is configured properly.'
                      )
                      version = f.read(8)
                      assert struct.unpack('<Q', version) == (1,)
                      code, self.element_size = struct.unpack('<QQ', f.read(16))
                      self.dtype = dtypes[code]
                      self._len, self.s = struct.unpack('<QQ', f.read(16))
                      self.doc_count = struct.unpack('<Q', f.read(8))
                      self.dim_offsets = read_longs(f, self._len + 1)
                      self.data_offsets = read_longs(f, self._len + 1)
                      self.sizes = read_longs(f, self.s)
                      self.doc_idx = read_longs(f, self.doc_count)
              
              def read_data(self, path):
              	self.data_file = open(data_file_path(path), 'rb', buffering=0)
              ```

          - cached：IndexedCachedDataset（IndexedDataset）

            - 封装了cache_index字典，预取

            ```python
                self.cache_index = {}
            
                def prefetch(self, indices):
                    if all(i in self.cache_index for i in indices):
                        return
                    if not self.data_file:
                        self.read_data(self.path)
                    indices = sorted(set(indices))
                    total_size = 0
                    for i in indices:
                        total_size += self.data_offsets[i + 1] - self.data_offsets[i]
                    self.cache = np.empty(total_size, dtype=self.dtype)
                    ptx = 0
                    self.cache_index.clear()
                    for i in indices:
                        self.cache_index[i] = ptx
                        size = self.data_offsets[i + 1] - self.data_offsets[i]
                        a = self.cache[ptx: ptx + size]
                        self.data_file.seek(self.data_offsets[i] * self.element_size)
                        self.data_file.readinto(a)
                        ptx += size
                    if self.data_file:
                        # close and delete data file after prefetch so we can pickle
                        self.data_file.close()
                        self.data_file = None
            ```

          - mmap：MMapIndexedDataset （IndexedDataset）

            - 主要成员：_path, _index, _bin_buffer， _bin_buffer_mmap

            ```python
            self._bin_buffer_mmap = np.memmap(data_file_path(self._path), mode='r', order='C')
                    
            self._bin_buffer = memoryview(self._bin_buffer_mmap) 
            ```

            - [memoryview](https://www.runoob.com/python/python-func-memoryview.html):函数返回给定参数的内存查看对象(memory view),所谓内存查看对象，是指对支持缓冲区协议的数据进行包装，在不需要复制对象基础上允许Python代码访问
              - 对内存地址的直接访问，char *p = data; 
            - 主要函数：\_do\_init，write，get，\_\_del\_\_，\__setstate__等等
            - get：从数据集中检索单个项，并带有只返回该项的一部分的选项。  Get (idx)与[idx]相同，不支持切片

            ```python
                def get(self, idx, offset=0, length=None):
                    ptr, size = self._index[idx]
                    if length is None:
                        length = size - offset
                    ptr += offset * np.dtype(self._index.dtype).itemsize
                    np_array = np.frombuffer(self._bin_buffer, dtype=self._index.dtype,
                                             count=length, offset=ptr)
                    return np_array
            ```

            - frombuffer：将缓冲区转化为为一维数组

    - get_train_valid_test_split_：从逗号或'/'分隔的字符串列表，获取数据集split

    - build_dataset：EncDecDataset 数据集处理

      - _build_index_mappings

        

### CPM-2-Pretrain模型

CPM-2: Large-scale Cost-effective Pre-trained Language Models

- AI Open会议，清华大学计算所，pre-trained language models  

- 代码：https://github.com/TsinghuaAI/CPM-2-Pretrain

- 背景及问题：训练更大的模型是深度学习的一个重要研究方向，近年来，预训练已成为开发大规模神经网络的主流技术。预训练语言模型(PLMs)的规模突飞猛进，这些大规模plm的**效率问题**限制了它们在现实场景中的使用

  - 预训练：其做法一般是将大量低成本收集的训练数据放在一起，经过某种预训方法去学习其中的共性，然后将其中的共性“移植”到特定任务的模型中，再使用相关特定领域的少量标注数据进行“微调”，这样的话，模型只需要从”共性“出发，去“学习”该特定任务的“特殊”部分即可
  - 使用 PLM 的成本随着模型大小的增长而迅速增加，并且对于大多数用户和研究人员来说变得无法承受。主要成本包括：
    - **预训练计算量大**：超大型模型需要数周的数千个 GPU 的预训练
    - **微调模型的存储成本大**：超大模型通常需要数百 GB 来存储，我们需要存储与下游任务一样多的模型
    - **推理设备要求高**：超大型模型推理通常需要多块GPU，计算资源有限，难以使用这些模型

- 优化措施

  - 使用**knowledge inheritance**方法（知识继承）：是利用现有PLM的知识来帮助新模型的预训练
  - 使用**prompt tuning** （提示调优）而不是微调fine-tuning 来减少特定于任务的参数的存储
  - 设计了一个高性能、内存高效的**推理框架INFMOE**，并采用了动态调度的卸载策略

- 设计：CPM-2-Pretrain

  - 基于优化后的PLM流程，开发了两种大规模成本-高效的预训练语言模型(Cost-efficient Pre-trained language Models, CPM-2)，一种是包含110亿个参数的中英双语模型，另一种是包含198亿个参数的专家混合模型 Mixture-of-Experts (MoE)  

  - 模型：由双向编码器和单向解码器组成的标准Transformer架构

    - 背景：加入Attention机制的Seq2seq模型（Encoder–Decoder 结构）在各个任务上都有了提升，现在的seq2seq模型指的都是结合RNN和attention的模型

      <img src="..\..\photos\RL\image-20210611144619383.png" alt="image-20210611144619383" style="zoom:33%;" />               <img src="..\..\photos\RL\Attention.png" alt="Attention" style="zoom:15%;" />

    - google提出了解决Seq2Seq问题的**Transformer**模型，用全attention的结构代替了lstm，在翻译任务上取得了更好的成绩

    - 编码/解码器：a variant of Masked Language Model (MLM)   

    - 并行训练：将attentiom和前向层沿宽度维度（width dimension）进行划分，最后将一个模型的分区分布在4个gpu上

    - 为了减少内存需求和加快训练前的速度，我们使用mixed-precision training、gradient checkpointing和ZERO-stage-1  

    - 对于CPM-2-MoE，将每个Transformer块的前馈层扩展到多个experts 。在向前传递过程中，对于每个token  ，根据其当前隐藏状态选择一个具有门控功能的experts。另外使用BASE的规划方法来平衡experts 选择层

      - 背景：多专家模型Mixture-of-Experts，MOE，略

  - 数据处理：分词，清洗，标准化，特征提取，建模 

    - 通过sentencepiece  方法减少句子冗余
    - sentencepiece  分词器会在分词序列中插入许多多余的空白标记“_”，这使得序列变得更长。使用WoBERT 的方法，我们用一个简单的前缀匹配替换了句子标记器，并删除了空白插入。新实现的分词器比句子分词器更有效、更易于使用

  - Pre-Training with Knowledge Inheritance 

    - 三阶段：Chinese pre-training, bilingual（双语） pre-training, and MoE pre-training  
    - 与从头开始的训练模型相比，具有知识继承的多阶段训练可以显著降低计算成本
    - Chinese pre-training：只使用中文文本作为训练数据。该模型可以集中学习汉语信息，具有很好的推广到其他语言的基础
    - bilingual  pre-training：在这一阶段，进一步从中文阶段对模型进行预训练中英文文本。优化包括前缀初始化，英中1：2
    - MoE pre-training ：多次复制双语阶段的模型来初始化MoE模型。对于门控网络，采用随机投影作为局部敏感哈希函数，在这一阶段不更新门控网络

- INFMOE: Memory-Efficient Inference Framework for MoE Layer  
  - PyTorch和TensorFlow是工业和学术界广泛使用的深度学习框架，用于训练和推理。还有许多其他框架，比如TensorRT和ONNX Runtime，它们是专门为不同设备上的高效模型推理而设计的。但由于各种原因，目前还不适合对MoE层进行有效的推理
  - https://github.com/TsinghuaAI/InfMoE
  
  

## 第二代NLP/SenseLM训练框架

- Transfer Learning in NLP

- 测试环境：1424集群

  - 模型文件夹：/mnt/cache/ladata/yangdong1/models，可以在里面找模型。
  - TFDS数据集文件夹：export TFDS_DATA_DIR=/mnt/cache/ladata/yangdong1/tensorflow_datasets，理论上有了它就不用联网了。当然实际加载数据还是按照 FLAN 的 seqio 任务名来的。
  - 加完 prompt 的数据集缓存文件夹：/mnt/cache/ladata/yangdong1/cache，可以极大提升预处理速度。现在已经预处理了所有能下载到的 FLAN 数据。
  - 1424 集群未联网，t5 库会强制用到 tf.io.gfile 导致 T5 tokenizer 无法加载。我们只需要把 t5 库中的 t5.data.util.py 中的 DEFAULT_SPM_PATH 改成缓存过的 <本仓库路径>/tokenizer_xxl/spiece.model 即可

- 运行脚本

  - ```
    python  -m torch.distributed.launch --nnodes ${NUM_WORKERS} --node_rank ${NODE_RANK} --nproc_per_node ${NUM_GPUS_PER_WORKER} --master_addr ${MASTER_ADDR}  --master_port ${MASTER_PORT}   run.py
    --deepspeed experiments/${SLURM_JOBID}/deepspeed_config.json
    --model_name_or_path ./models
    --per_device_train_batch_size 16 --per_device_eval_batch_size 32 
    --output_dir experiments/${SLURM_JOBID}/model
    --learning_rate 0.001 --lr constant 
    --max_source_length 1024 --max_target_length 256
    --do_train  --do_eval --num_train_epochs 100 --max_steps 50000
    --dataset_name glue_mix.flan --save strategy steps --save_steps 500 --save_total_limit 1
    --gradient_checkpointing true --gradient_accumulation_steps 8
    --logging_steps 8  --eval_steps 1000 --evaluation_strategy steps  --max_eval_samples 10240
    --overwrite_output_dir True
    ```

- 代码分析

  - 主要建立在 [Huggingface/ transformers](https://github.com/huggingface/transformers)

    - 使用 DeepSpeed 进行分布式训练
    - 基于seqio的IO和评价框架。支持在不同的数据集及其混合物上进行预训练/微调模型，并在各种基准上进行评估，例如GLUE、SuperGLUE、CLUE(WIP)等
    - 以 Huggingface 格式加载/训练/保存语言模型

    - Prompt tuning & transfer
      - Finetuned Language Models Are Zero-Shot Learners，ICLR‘22，Google Research
        - 微调语言网络 (fine-tuned language net, **FLAN**)
      - Multitask prompted training enables zero-shot task generalization，Huggingface 

  - 背景补充—NLP发展

    - Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing，46页

      <img src="..\..\photos\paper\image-20220215145306242.png" alt="image-20220215145306242" style="zoom:50%;" />

    - 第三范式[**预训练、微调范式**]：传统预训练pre-train, fine-tune，即将PLM应用到下游任务上，在预训练阶段和微调阶段根据下游任务设计训练对象并对PLM本体进行调整

    - 第四范式[**预训练、prompt 和预测**]：提示学习/调优 Prompt learning/tuning：在这种范式中，不是通过目标工程使预训练的语言模型（LM）适应下游任务，而是重新形式化（Reformulate）下游任务，使其看起来更像是在文本 prompt 的帮助下在原始 LM 训练期间解决的任务。通过这种方式，选择适当的 prompt，该方法可以操纵模型的行为，以便预训练的 LM 本身可以用于预测所需的输出，有时甚至无需任何额外的特定任务训练。这种方法的优点是给定一组合适的 prompt，以完全无监督的方式训练的单个 LM 就能够用于解决大量任务

      - 论文列表： https://github.com/thunlp/PromptPapers

    - Prompt扩展：迁移Prompt，Multi-Prompt Learning

      ![image-20220215145649117](..\..\photos\paper\image-20220215145649117.png)

  - 数据集

    - 统一NLP数据集text-to-text格式，使用tensorflow_datasets和seqio框架以基于任务的方式处理数据集，统一了预处理、后处理和自动计算
    - 任务Tasks
      - T5-style数据集： super_glue_cb_v102, starts with the name of the task, followed by concatenating the inputs)
      - FLAN指令调优数据集：an instruction prompt style (describe the task in natural instructions, and concatenate the inputs in natural language).
    - 混合数据：定义了几个数据集混合，将相似的任务聚合在一起进行训练
    -  SuperGLUE 基准测试

  - 训练主流程：于[Huggingface/ transformers](https://github.com/huggingface/transformers)一致，参考[transformers文档](https://huggingface.co/docs/transformers/model_sharing)

    - ModelArguments，DataTrainingArguments，Seq2SeqTrainingArguments

  - 数据模块：

    - seqio： IO & evaluation framework，用于训练和评测**数据集**
    - Load/Train/Save Language models in Huggingface format，用于**模型**
      - The **tokenizer** is responsible for all the preprocessing the pretrained model expects, and can be called directly on a single string (as in the above examples) or a list. It will output a **dictionary** that you can use in downstream code or simply directly pass to your model using the ** **argument unpacking operator.**
        - 负责预训练模型中所有预处理，并且可以直接在单个字符串或列表上调用
        - 输出一个字典，可以在下游代码中使用该字典，或者使用 ** 参数解包运算符直接传递给模型
      - tokenizer，return_tensors支持 "pt" 或者 "tf"
      - The model itself is a regular **Pytorch nn.Module** or a **TensorFlow tf.keras.Model** (depending on your backend) which you can use normally. This tutorial explains how to integrate such a model into a classic PyTorch or TensorFlow training loop, or how to use our Trainer API to quickly fine-tune on a new dataset.

## 思考：What can we do？

1. 他们的工作也是正起步阶段，尚不明确要做成什么样子（Megatron-DeepSpeed，DeepSpeed尝试）
   - 他们不知道做什么，也不知道要我们做什么，可能要一同研究
2. 我们的工作不明确，后续计划？
   - IO成为瓶颈？：分析流程，测试瓶颈
     - 问题：数据集怎么办，他们代码随时会改
   - 大模型并行训练和卸载？
     - 问题：训练流程和方法尚不明确，要由他们主导
   - Ceph / Lustre会影响性能：优化ceph/lustre的使用
     - 问题：会不会用到Ceph/lustre？